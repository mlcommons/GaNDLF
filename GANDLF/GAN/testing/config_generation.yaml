problem_type: synthesis
track_memory_usage : True
version:
  {
    minimum: 0.0.14,
    maximum: 0.0.18
  }
model:
  {
    latent_vector_size: 100,
    dimension: 2, # the dimension of the model and dataset: defines dimensionality of computations
    base_filters: 32, # Set base filters: number of filters present in the initial module of the U-Net convolution; for IncU-Net, keep this divisible by 4
    architecture: dcgan, # options: unet, resunet, fcn, uinc
    final_layer: sigmoid, # can be either sigmoid, softmax or none (none == regression)
    norm_type: batch, # can be either batch or instance
    class_list: [0,255], # Set the list of labels the model should train on and predict
    amp: False, # Set if you want to use Automatic Mixed Precision for your operations or not - options: True, False
    save_every_n_epochs: 10, # Set the frequency of saving the model
    # n_channels: 3, # set the input channels - useful when reading RGB or images that have vectored pixel types
  }
metrics:
  - ssim
  # - fid
  - lpips
metrics_config:
  {
    ssim: {
      "reduction": none},
    fid : {
      "features_size": 2048,
      },
    lpips: {
      "net_type": squeeze,
      "reduction":mean,
      "converter_type": soft
    }
  }

verbose: True
inference_mechanism: {
  grid_aggregator_overlap: average,
  patch_overlap: 0,
}
modality: rad
# Patch size during training - 2D patch for breast images since third dimension is not patched 
patch_size: [64,64]
# Number of epochs
num_epochs: 1
patience: 1
# Set the batch size
batch_size: 128
# Set the initial learning rate
learning_rate_g: 0.0002
learning_rate_d: 0.0002
# Set the learning rate scheduler i.e. the way the initial learning rate must be updated while the training progresses
# Options: steplr, exponentiallr, cosineannealinglr, reducelronplateau, cycliclr
random_seed_validation: 42
random_seed_testing: 42
save_output: True
save_grid : True
save_training : False
scheduler_d: triangle
scheduler_g: triangle
# Set which loss function you want to use - options : 'dc' - for dice only, 'dcce' - for sum of dice and CE and you can guess the next (only lower-case please)
# options: dc (dice only), ce (), dcce (sume of dice and ce), mse (), ...
loss_function: ce
weighted_loss: False
# Which optimizer do you want to use - adam/sgd
optimizer_d: {type: adam, betas : [0.5, 0.999]}
optimizer_g: {type: adam, betas : [0.5, 0.999]}

# the value of 'k' for cross-validation, this is the percentage of total training data to use as validation;
# randomized split is performed using sklearn's KFold method
# for single fold run, use '-' before the fold number
nested_training:
  {
    testing: -5, # this controls the holdout data splits for final model evaluation; use '1' if this is to be disabled
    validation: -5 # this controls the validation data splits for model training
  }
# various data augmentation techniques
# options: affine, elastic, downsample, motion, ghosting, bias, blur, gaussianNoise, swap
# keep/edit as needed
# all transforms: https://torchio.readthedocs.io/transforms/transforms.html?highlight=transforms
data_augmentation:
  {
  # 'spatial':{
  #   'probability': 0.5
  # },
  # 'kspace':{
  #   'probability': 0.5
  # },
  # 'bias':{
  #   'probability': 0.5
  # },
  # 'blur':{
  #   'probability': 0.5
  # },
  # 'noise':{
  #   'probability': 0.5
  # },
  # 'swap':{
  #   'probability': 0.5
  # }
  }
data_preprocessing:
  {
    # 'threshold':{
    #   'min': 10, 
    #   'max': 75
    # },
    # 'clip':{
    #   'min': 10, 
    #   'max': 75
    # }, 
    # 'normalize_standardize',
    # 'resample':{
    #   'resolution': [1,2,3]
    # },
    # 'resize': [64,64], # this is generally not recommended, as it changes image properties in unexpected ways
  }
# data postprocessing node
data_postprocessing:
  {
    # 'largest_component',
    # 'hole_filling'
  }
# parallel training on HPC - here goes the command to prepend to send to a high performance computing
# cluster for parallel computing during multi-fold training
# not used for single fold training
# this gets passed before the training_loop, so ensure enough memory is provided along with other parameters
# that your HPC would expect
# ${outputDir} will be changed to the outputDir you pass in CLI + '/${fold_number}'
#parallel_compute_command: <insert parallel command here>

q_max_length: 1

q_samples_per_volume: 1

q_num_workers: 0

inference_config: {
  n_generated_samples : 12, # number of samples to generate
  batch_size: 6, # batch size for inference
  save_format : "png"
  }
validation_config: {
  n_generated_samples : 12, # number of samples to generate
  batch_size: 12, # batch size for inference
  }
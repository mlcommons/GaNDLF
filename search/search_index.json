{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GaNDLF \u00b6 The G ener a lly N uanced D eep L earning F ramework (GaNDLF) for reproducible segmentation and classification. Why use GaNDLF? \u00b6 GaNDLF was developed to lower the barrier to AI, enabling reproducibility, translation, and deployment. As an out-of-the-box solution, GaNDLF alleviates the need to build from scratch. Users may kickstart their project by modifying only a configuration (config) file that provides guidelines for the envisioned pipeline and CSV inputs that describe the training data. Range of GaNDLF functionalities: \u00b6 Supports multiple Deep Learning model architectures Channels/modalities Prediction classes Robust data augmentation, courtesy of TorchIO and Albumentations Built-in cross validation, with support for parallel HPC-based computing Multi-GPU (on the same machine) training Leverages robust open source software Zero -code needed to train robust models Low -code requirement for customization Automatic mixed precision support Table of Contents \u00b6 Getting Started Application Setup Usage Customize the training and inference Extending GaNDLF ITCR Connectivity FAQ Acknowledgements Citation \u00b6 Please cite the following article for GaNDLF ( full paper ): @article { pati2023gandlf , author = {Pati, Sarthak and Thakur, Siddhesh P. and Hamamc{\\i}, {\\.{I}}brahim Ethem and Baid, Ujjwal and Baheti, Bhakti and Bhalerao, Megh and G{\\\"u}ley, Orhun and Mouchtaris, Sofia and Lang, David and Thermos, Spyridon and Gotkowski, Karol and Gonz{\\'a}lez, Camila and Grenko, Caleb and Getka, Alexander and Edwards, Brandon and Sheller, Micah and Wu, Junwen and Karkada, Deepthi and Panchumarthy, Ravi and Ahluwalia, Vinayak and Zou, Chunrui and Bashyam, Vishnu and Li, Yuemeng and Haghighi, Babak and Chitalia, Rhea and Abousamra, Shahira and Kurc, Tahsin M. and Gastounioti, Aimilia and Er, Sezgin and Bergman, Mark and Saltz, Joel H. and Fan, Yong and Shah, Prashant and Mukhopadhyay, Anirban and Tsaftaris, Sotirios A. and Menze, Bjoern and Davatzikos, Christos and Kontos, Despina and Karargyris, Alexandros and Umeton, Renato and Mattson, Peter and Bakas, Spyridon} , title = {GaNDLF: the generally nuanced deep learning framework for scalable end-to-end clinical workflows} , journal = {Communications Engineering} , year = {2023} , month = {May} , day = {16} , volume = {2} , number = {1} , pages = {23} , issn = {2731-3395} , doi = {10.1038/s44172-023-00066-3} , url = {https://doi.org/10.1038/s44172-023-00066-3} } Contact \u00b6 GaNDLF developers can be reached via the following ways: GitHub Discussions Email","title":"Home"},{"location":"#gandlf","text":"The G ener a lly N uanced D eep L earning F ramework (GaNDLF) for reproducible segmentation and classification.","title":"GaNDLF"},{"location":"#why-use-gandlf","text":"GaNDLF was developed to lower the barrier to AI, enabling reproducibility, translation, and deployment. As an out-of-the-box solution, GaNDLF alleviates the need to build from scratch. Users may kickstart their project by modifying only a configuration (config) file that provides guidelines for the envisioned pipeline and CSV inputs that describe the training data.","title":"Why use GaNDLF?"},{"location":"#range-of-gandlf-functionalities","text":"Supports multiple Deep Learning model architectures Channels/modalities Prediction classes Robust data augmentation, courtesy of TorchIO and Albumentations Built-in cross validation, with support for parallel HPC-based computing Multi-GPU (on the same machine) training Leverages robust open source software Zero -code needed to train robust models Low -code requirement for customization Automatic mixed precision support","title":"Range of GaNDLF functionalities:"},{"location":"#table-of-contents","text":"Getting Started Application Setup Usage Customize the training and inference Extending GaNDLF ITCR Connectivity FAQ Acknowledgements","title":"Table of Contents"},{"location":"#citation","text":"Please cite the following article for GaNDLF ( full paper ): @article { pati2023gandlf , author = {Pati, Sarthak and Thakur, Siddhesh P. and Hamamc{\\i}, {\\.{I}}brahim Ethem and Baid, Ujjwal and Baheti, Bhakti and Bhalerao, Megh and G{\\\"u}ley, Orhun and Mouchtaris, Sofia and Lang, David and Thermos, Spyridon and Gotkowski, Karol and Gonz{\\'a}lez, Camila and Grenko, Caleb and Getka, Alexander and Edwards, Brandon and Sheller, Micah and Wu, Junwen and Karkada, Deepthi and Panchumarthy, Ravi and Ahluwalia, Vinayak and Zou, Chunrui and Bashyam, Vishnu and Li, Yuemeng and Haghighi, Babak and Chitalia, Rhea and Abousamra, Shahira and Kurc, Tahsin M. and Gastounioti, Aimilia and Er, Sezgin and Bergman, Mark and Saltz, Joel H. and Fan, Yong and Shah, Prashant and Mukhopadhyay, Anirban and Tsaftaris, Sotirios A. and Menze, Bjoern and Davatzikos, Christos and Kontos, Despina and Karargyris, Alexandros and Umeton, Renato and Mattson, Peter and Bakas, Spyridon} , title = {GaNDLF: the generally nuanced deep learning framework for scalable end-to-end clinical workflows} , journal = {Communications Engineering} , year = {2023} , month = {May} , day = {16} , volume = {2} , number = {1} , pages = {23} , issn = {2731-3395} , doi = {10.1038/s44172-023-00066-3} , url = {https://doi.org/10.1038/s44172-023-00066-3} }","title":"Citation"},{"location":"#contact","text":"GaNDLF developers can be reached via the following ways: GitHub Discussions Email","title":"Contact"},{"location":"acknowledgements/","text":"Acknowledgements \u00b6 This file records the following pieces of information: All papers whose results were reproduced for the GaNDLF manuscript . All challenges (links and references) that various developers/users participated in with GaNDLF. All papers that used GaNDLF in their analysis. Papers \u00b6 Application Lead Author Link Brain Extraction Siddhesh Thakur DOI:10.1016/j.neuroimage.2020.117081 Brain Tumor Segmentation Megh Bhalerao DOI:10.1007/978-3-030-46643-5_21 Brain Age Prediction Vishnu Bashyam DOI:10.1093/brain/awaa160 Dental Segmentation At\u0131f Emre Y\u00fcksel DOI:10.1038/s41598-021-90386-1 Challenges \u00b6 Challenge Name Year Host Conference Link Pathologic Myopia 2019 ISBI palm.grand-challenge.org DigestPath 2019 MICCAI digestpath2019.grand-challenge.org Diabetic Foot Ulcer 2021 MICCAI dfu-2021.grand-challenge.org People \u00b6 All coders and developers of GaNDLF Supervisors: Aimilia Gastounioti Mark Bergman Yong Fan Anirban Mukhopadhyay Sotirios Tsaftaris Bjoern Menze Despina Kontos Christos Davatzikos Spyridon Bakas","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"This file records the following pieces of information: All papers whose results were reproduced for the GaNDLF manuscript . All challenges (links and references) that various developers/users participated in with GaNDLF. All papers that used GaNDLF in their analysis.","title":"Acknowledgements"},{"location":"acknowledgements/#papers","text":"Application Lead Author Link Brain Extraction Siddhesh Thakur DOI:10.1016/j.neuroimage.2020.117081 Brain Tumor Segmentation Megh Bhalerao DOI:10.1007/978-3-030-46643-5_21 Brain Age Prediction Vishnu Bashyam DOI:10.1093/brain/awaa160 Dental Segmentation At\u0131f Emre Y\u00fcksel DOI:10.1038/s41598-021-90386-1","title":"Papers"},{"location":"acknowledgements/#challenges","text":"Challenge Name Year Host Conference Link Pathologic Myopia 2019 ISBI palm.grand-challenge.org DigestPath 2019 MICCAI digestpath2019.grand-challenge.org Diabetic Foot Ulcer 2021 MICCAI dfu-2021.grand-challenge.org","title":"Challenges"},{"location":"acknowledgements/#people","text":"All coders and developers of GaNDLF Supervisors: Aimilia Gastounioti Mark Bergman Yong Fan Anirban Mukhopadhyay Sotirios Tsaftaris Bjoern Menze Despina Kontos Christos Davatzikos Spyridon Bakas","title":"People"},{"location":"customize/","text":"This file contains mid-level information regarding various parameters that can be leveraged to customize the training/inference in GaNDLF. Model \u00b6 Defined under the global key model in the config file architecture : Defines the model architecture (aka \"network topology\") to be used for training. All options can be found here . Some examples are: Segmentation: Standardized 4-layer UNet with ( resunet ) and without ( unet ) residual connections, as described in this paper . Multi-layer UNet with ( resunet_multilayer ) and without ( unet_multilayer ) residual connections - this is a more general version of the standard UNet, where the number of layers can be specified by the user. UNet with Inception Blocks ( uinc ) is a variant of UNet with inception blocks, as described in this paper . UNetR ( unetr ) is a variant of UNet with transformers, as described in this paper . TransUNet ( transunet ) is a variant of UNet with transformers, as described in this paper . And many more. Classification/Regression: VGG configurations ( vgg11 , vgg13 , vgg16 , vgg19 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). VGG configurations initialized with weights trained on ImageNet ( imagenet_vgg11 , imagenet_vgg13 , imagenet_vgg16 , imagenet_vgg19 ), as described in this paper . DenseNet configurations ( densenet121 , densenet161 , densenet169 , densenet201 , densenet264 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). ResNet configurations ( resnet18 , resnet34 , resnet50 , resnet101 , resnet152 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). And many more. dimension : Defines the dimensionality of convolutions, this is usually the same dimension as the input image, unless specialized processing is done to convert images to a different dimensionality (usually not recommended). For example, 2D images can be stacked to form a \"pseudo\" 3D image, and 3D images can be processed as \"slices\" as 2D images. final_layer : The final layer of model that will be used to generate the final prediction. Unless otherwise specified, it can be one of softmax or sigmoid or logits or none (the latter 2 are only used for regression tasks). class_list : The list of classes that will be used for training. This is expected to be a list of integers. For example, for a segmentation task, this can be a list of integers [0, 1, 2, 4] for the BraTS training case for all labels (background, necrosis, edema, and enhancing tumor). Additionally, different labels can be combined to perform \"combinatorial training\", such as [0, 1||4, 1||2||4, 4] , for the BraTS training to train on background, tumor core, whole tumor, and enhancing, respectively. For a classification task, this can be a list of integers [0, 1] . ignore_label_validation : This is the location of the label in class_list whose performance is to be ignored during metric calculation for validation/testing data norm_type : The type of normalization to be used. This can be either batch or instance or none . Various other options specific to architectures, such as (but not limited to): densenet models: growth_rate : how many filters to add each layer (k in paper) bn_size : multiplicative factor for number of bottle neck layers # (i.e. bn_size * k features in the bottleneck layer) drop_rate : dropout rate after each dense layer unet_multilayer and other networks that support multiple layers: depth : the number of encoder/decoder (or other types of) layers Loss function \u00b6 Defined in the loss_function parameter of the model configuration. By passing weighted_loss: True , the loss function will be weighted by the inverse of the class frequency. This parameter controls the function which the model is trained. All options can be found here . Some examples are: Segmentation: dice ( dice or dc ), dice and cross entropy ( dcce ), focal loss ( focal ), dice and focal ( dc_focal ), matthews ( mcc ) Classification/regression: mean squared error ( mse ) And many more. Metrics \u00b6 Defined in the metrics parameter of the model configuration. This parameter controls the metrics to be used for model evaluation for the training/validation/testing datasets. All options can be found here . Most of these metrics are calculated using TorchMetrics . Some examples are: Segmentation: dice ( dice and dice_per_label ), hausdorff distances ( hausdorff or hausdorff100 and hausdorff100_per_label ), hausdorff distances including on the 95th percentile of distances ( hausdorff95 and hausdorff95_per_label ) - Classification/regression: mean squared error ( mse ) calculated per sample Metrics calculated per cohort (these are automatically calculated for classification and regression and cannot be disabled ): Classification: accuracy, precision, recall, f1, for the entire cohort (\"global\"), per classified class (\"per_class\"), per classified class averaged (\"per_class_average\"), per classified class weighted/balanced (\"per_class_weighted\") Regression: mean absolute error, pearson and spearman coefficients, calculated as mean, sum, or standard. Patching Strategy \u00b6 patch_size : The size of the patch to be used for training. This is expected to be a list of integers, with the length of the list being the same as the dimensionality of the input image. For example, for a 2D image, this can be [128, 128] , and for a 3D image, this can be [128, 128, 128] . patch_sampler : The sampler to be used for patch sampling during training. This can be one of uniform (the entire input image has equal weight on contributing a valid patch) or label (only the regions that have a valid ground truth segmentation label can contribute a patch). label sampler usually requires padding of the image to ensure blank patches are not inadvertently sampled; this can be controlled by the enable_padding parameter. inference_mechanism grid_aggregator_overlap : this option provides the option to strategize the grid aggregation output; should be either crop or average - https://torchio.readthedocs.io/patches/patch_inference.html#grid-aggregator patch_overlap : the amount of overlap of patches during inference in terms of pixels, defaults to 0 ; see https://torchio.readthedocs.io/patches/patch_inference.html#gridsampler for details. Data Preprocessing \u00b6 Defined in the data_preprocessing parameter of the model configuration. This parameter controls the various preprocessing functions that are applied to the entire image before the patching strategy is applied. All options can be found here . Some of the most important examples are: Intensity harmonization : GaNDLF provides multiple normalization and rescaling options to ensure intensity-level harmonization of the entire cohort. Some examples include: normalize : simple Z-score normalization normalize_positive : this performs z-score normalization only on pixels > 0 normalize_nonZero : this performs z-score normalization only on pixels != 0 normalize_nonZero_masked : this performs z-score normalization only on the region defined by the ground truth annotation rescale : simple min-max rescaling, sub-parameters include in_min_max , out_min_max , percentiles ; this option is useful to discard outliers in the intensity distribution Template-based normalization: These options take a target image as input (defined by the target sub-parameter) and perform different matching strategies to match input image(s) to this target. histogram_matching : this performs histogram matching as defined by this paper . If the target image is absent, this will perform global histogram equalization. If target is adaptive , this will perform adaptive histogram equalization . stain_normalization : these are normalization techniques specifically designed for histology images; the different options include vahadane , macenko , or ruifrok , under the extractor sub-parameter. Always needs a target image to work. Resolution harmonization : GaNDLF provides multiple resampling options to ensure resolution-level harmonization of the entire cohort. Some examples include: resample : resamples the image to the specified by the resolution sub-parameter resample_min : resamples the image to the maximum spacing defined by the resolution sub-parameter; this is useful in cohorts that have varying resolutions, but the user wants to resample to the minimum resolution for consistency resize_image : NOT RECOMMENDED ; resizes the image to the specified size resize_patch : NOT RECOMMENDED ; resizes the extracted patch to the specified size And many more. Data Augmentation \u00b6 Defined in the data_augmentation parameter of the model configuration. This parameter controls the various augmentation functions that are applied to the entire image before the patching strategy is applied. These should be defined in cognition of the task at hand (for example, RGB augmentations will not work for MRI/CT and other similar radiology images). All options can contain a probability sub-parameter, which defines the probability of the augmentation being applied to the image. When present, this will supersede the default_probability parameter. All options can be found here . Some of the most important examples are: Radiology-specific augmentations kspace : one of either ghosting or spiking is picked for augmentation. bias : applies a random bias field artefact to the input image using this function . RGB-specific augmentations colorjitter : applies the ColorJitter transform from PyTorch, has sub-parameters brightness , contrast , saturation , and hue . General-purpose augmentations Spatial transforms : they only change the resolution (and thereby, the shape) of the input image, and only apply interpolation to the intensities for consistency affine : applies a random affine transformation to the input image; for details, see this page ; has sub-parameters scales (defining the scaling ranges), degrees (defining the rotation ranges), and translation (defining the translation ranges in real-world coordinates , which is usually in mm ) elastic : applies a random elastic deformation to the input image; for details, see this page ; has sub-parameters num_control_points (defining the number of control points), locked_borders (defining the number of locked borders), max_displacement (defining the maximum displacement of the control points), num_control_points (defining the number of control points), and locked_borders (defining the number of locked borders). flip : applies a random flip to the input image; for details, see this page ; has sub-parameter axes (defining the axes to flip). rotate : applies a random rotation by 90 degrees ( rotate_90 ) or 180 degrees ( rotate_180 ), has sub-parameter axes (defining the axes to rotate). swap : applies a random swap , has sub-parameter patch_size (defining the patch size to swap), and num_iterations (number of iterations that 2 patches will be swapped). Intensity transforms : they change the intensity of the input image, but never the actual resolution or shape. motion : applies a random motion blur to the input image using this function . blur : applies a random Gaussian blur to the input image using this function l has sub-parameter std (defines the standard deviation range). noise : applies a random noise to the input image using this function ; has sub-parameters std (defines the standard deviation range) and mean (defines the mean of the noise to be added). noise_var : applies a random noise to the input image, however, the with default std = [0, 0.015 * std(image)] . anisotropic : applies random anisotropic transform to input image using this function . This changes the resolution and brings it back to its original resolution, thus applying \"real-world\" interpolation to images. Training Parameters \u00b6 These are various parameters that control the overall training process. verbose : generate verbose messages on console; generally used for debugging. batch_size : batch size to be used for training. in_memory : this is to enable or disable lazy loading. If set to True , all data is loaded onto the RAM at once during the construction of the dataloader (either training/validation/testing), thus resulting in faster training. If set to False , data gets read into RAM on-the-go when needed (also called \"lazy loading\" ), which slows down training but lessens the memory load. The latter is recommended if the user's RAM has limited capacity. num_epochs : number of epochs to train for. patience : number of epochs to wait for improvement in the validation loss before early stopping. learning_rate : learning rate to be used for training. scheduler : learning rate scheduler to be used for training, more details are here ; can take the following sub-parameters: type : triangle , triangle_modified , exp , step , reduce-on-plateau , cosineannealing , triangular , triangular2 , exp_range min_lr : minimum learning rate to be used for training. max_lr : maximum learning rate to be used for training. optimizer : optimizer to be used for training, more details are here . nested_training : number of folds to use nested training, takes testing and validation as sub-parameters, with integer values defining the number of folds to use. memory_save_mode : if enabled, resize/resample operations in data_preprocessing will save files to disk instead of directly getting read into memory as tensors Queue configuration : this defines how the queue for the input to the model is to be designed after the patching strategy has been applied, and more details are here . This takes the following sub-parameters: q_max_length : his determines the maximum number of patches that can be stored in the queue. Using a large number means that the queue needs to be filled less often, but more CPU memory is needed to store the patches. q_samples_per_volume : this determines the number of patches to extract from each volume. A small number of patches ensures a large variability in the queue, but training will be slower. q_num_workers : this determines the number subprocesses to use for data loading; '0' means main process is used, scale this according to available CPU resources. q_verbose : used to debug the queue Differentially Private Training \u00b6 GaNDLF supports training differentially private models using Opacus . Here are some resources using which one can train private models: TLDR on DP and private training: read this paper and this blog post . All options are present in a new key called differential_privacy in the config file. It has the following options: noise_multiplier : The ratio of the standard deviation of the Gaussian noise to the L2-sensitivity of the function to which the noise is added. max_grad_norm : The maximum norm of the per-sample gradients. Any gradient with norm higher than this will be clipped to this value. accountant : Accounting mechanism. Currently supported: rdp (RDPAccountant), gdp (GaussianAccountant), prv (PRVAccountant) secure_mode : Set to True if cryptographically strong DP guarantee is required. secure_mode=True uses secure random number generator for noise and shuffling (as opposed to pseudo-rng in vanilla PyTorch) and prevents certain floating-point arithmetic-based attacks. allow_opacus_model_fix : Enabled automated fixing of the model based on Opacus [ ref ] delta : Target delta to be achieved. Probability of information being leaked. Use either this or epsilon . epsilon : Target epsilon to be achieved, a metric of privacy loss at differential changes in data. Use either this or delta . physical_batch_size : The batch size to use for DP computation (it is usually set lower than the baseline or non-DP batch size). Defaults to batch_size .","title":"Customize Training and Inference"},{"location":"customize/#model","text":"Defined under the global key model in the config file architecture : Defines the model architecture (aka \"network topology\") to be used for training. All options can be found here . Some examples are: Segmentation: Standardized 4-layer UNet with ( resunet ) and without ( unet ) residual connections, as described in this paper . Multi-layer UNet with ( resunet_multilayer ) and without ( unet_multilayer ) residual connections - this is a more general version of the standard UNet, where the number of layers can be specified by the user. UNet with Inception Blocks ( uinc ) is a variant of UNet with inception blocks, as described in this paper . UNetR ( unetr ) is a variant of UNet with transformers, as described in this paper . TransUNet ( transunet ) is a variant of UNet with transformers, as described in this paper . And many more. Classification/Regression: VGG configurations ( vgg11 , vgg13 , vgg16 , vgg19 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). VGG configurations initialized with weights trained on ImageNet ( imagenet_vgg11 , imagenet_vgg13 , imagenet_vgg16 , imagenet_vgg19 ), as described in this paper . DenseNet configurations ( densenet121 , densenet161 , densenet169 , densenet201 , densenet264 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). ResNet configurations ( resnet18 , resnet34 , resnet50 , resnet101 , resnet152 ), as described in this paper . Our implementation allows true 3D computations (as opposed to 2D+1D convolutions). And many more. dimension : Defines the dimensionality of convolutions, this is usually the same dimension as the input image, unless specialized processing is done to convert images to a different dimensionality (usually not recommended). For example, 2D images can be stacked to form a \"pseudo\" 3D image, and 3D images can be processed as \"slices\" as 2D images. final_layer : The final layer of model that will be used to generate the final prediction. Unless otherwise specified, it can be one of softmax or sigmoid or logits or none (the latter 2 are only used for regression tasks). class_list : The list of classes that will be used for training. This is expected to be a list of integers. For example, for a segmentation task, this can be a list of integers [0, 1, 2, 4] for the BraTS training case for all labels (background, necrosis, edema, and enhancing tumor). Additionally, different labels can be combined to perform \"combinatorial training\", such as [0, 1||4, 1||2||4, 4] , for the BraTS training to train on background, tumor core, whole tumor, and enhancing, respectively. For a classification task, this can be a list of integers [0, 1] . ignore_label_validation : This is the location of the label in class_list whose performance is to be ignored during metric calculation for validation/testing data norm_type : The type of normalization to be used. This can be either batch or instance or none . Various other options specific to architectures, such as (but not limited to): densenet models: growth_rate : how many filters to add each layer (k in paper) bn_size : multiplicative factor for number of bottle neck layers # (i.e. bn_size * k features in the bottleneck layer) drop_rate : dropout rate after each dense layer unet_multilayer and other networks that support multiple layers: depth : the number of encoder/decoder (or other types of) layers","title":"Model"},{"location":"customize/#loss-function","text":"Defined in the loss_function parameter of the model configuration. By passing weighted_loss: True , the loss function will be weighted by the inverse of the class frequency. This parameter controls the function which the model is trained. All options can be found here . Some examples are: Segmentation: dice ( dice or dc ), dice and cross entropy ( dcce ), focal loss ( focal ), dice and focal ( dc_focal ), matthews ( mcc ) Classification/regression: mean squared error ( mse ) And many more.","title":"Loss function"},{"location":"customize/#metrics","text":"Defined in the metrics parameter of the model configuration. This parameter controls the metrics to be used for model evaluation for the training/validation/testing datasets. All options can be found here . Most of these metrics are calculated using TorchMetrics . Some examples are: Segmentation: dice ( dice and dice_per_label ), hausdorff distances ( hausdorff or hausdorff100 and hausdorff100_per_label ), hausdorff distances including on the 95th percentile of distances ( hausdorff95 and hausdorff95_per_label ) - Classification/regression: mean squared error ( mse ) calculated per sample Metrics calculated per cohort (these are automatically calculated for classification and regression and cannot be disabled ): Classification: accuracy, precision, recall, f1, for the entire cohort (\"global\"), per classified class (\"per_class\"), per classified class averaged (\"per_class_average\"), per classified class weighted/balanced (\"per_class_weighted\") Regression: mean absolute error, pearson and spearman coefficients, calculated as mean, sum, or standard.","title":"Metrics"},{"location":"customize/#patching-strategy","text":"patch_size : The size of the patch to be used for training. This is expected to be a list of integers, with the length of the list being the same as the dimensionality of the input image. For example, for a 2D image, this can be [128, 128] , and for a 3D image, this can be [128, 128, 128] . patch_sampler : The sampler to be used for patch sampling during training. This can be one of uniform (the entire input image has equal weight on contributing a valid patch) or label (only the regions that have a valid ground truth segmentation label can contribute a patch). label sampler usually requires padding of the image to ensure blank patches are not inadvertently sampled; this can be controlled by the enable_padding parameter. inference_mechanism grid_aggregator_overlap : this option provides the option to strategize the grid aggregation output; should be either crop or average - https://torchio.readthedocs.io/patches/patch_inference.html#grid-aggregator patch_overlap : the amount of overlap of patches during inference in terms of pixels, defaults to 0 ; see https://torchio.readthedocs.io/patches/patch_inference.html#gridsampler for details.","title":"Patching Strategy"},{"location":"customize/#data-preprocessing","text":"Defined in the data_preprocessing parameter of the model configuration. This parameter controls the various preprocessing functions that are applied to the entire image before the patching strategy is applied. All options can be found here . Some of the most important examples are: Intensity harmonization : GaNDLF provides multiple normalization and rescaling options to ensure intensity-level harmonization of the entire cohort. Some examples include: normalize : simple Z-score normalization normalize_positive : this performs z-score normalization only on pixels > 0 normalize_nonZero : this performs z-score normalization only on pixels != 0 normalize_nonZero_masked : this performs z-score normalization only on the region defined by the ground truth annotation rescale : simple min-max rescaling, sub-parameters include in_min_max , out_min_max , percentiles ; this option is useful to discard outliers in the intensity distribution Template-based normalization: These options take a target image as input (defined by the target sub-parameter) and perform different matching strategies to match input image(s) to this target. histogram_matching : this performs histogram matching as defined by this paper . If the target image is absent, this will perform global histogram equalization. If target is adaptive , this will perform adaptive histogram equalization . stain_normalization : these are normalization techniques specifically designed for histology images; the different options include vahadane , macenko , or ruifrok , under the extractor sub-parameter. Always needs a target image to work. Resolution harmonization : GaNDLF provides multiple resampling options to ensure resolution-level harmonization of the entire cohort. Some examples include: resample : resamples the image to the specified by the resolution sub-parameter resample_min : resamples the image to the maximum spacing defined by the resolution sub-parameter; this is useful in cohorts that have varying resolutions, but the user wants to resample to the minimum resolution for consistency resize_image : NOT RECOMMENDED ; resizes the image to the specified size resize_patch : NOT RECOMMENDED ; resizes the extracted patch to the specified size And many more.","title":"Data Preprocessing"},{"location":"customize/#data-augmentation","text":"Defined in the data_augmentation parameter of the model configuration. This parameter controls the various augmentation functions that are applied to the entire image before the patching strategy is applied. These should be defined in cognition of the task at hand (for example, RGB augmentations will not work for MRI/CT and other similar radiology images). All options can contain a probability sub-parameter, which defines the probability of the augmentation being applied to the image. When present, this will supersede the default_probability parameter. All options can be found here . Some of the most important examples are: Radiology-specific augmentations kspace : one of either ghosting or spiking is picked for augmentation. bias : applies a random bias field artefact to the input image using this function . RGB-specific augmentations colorjitter : applies the ColorJitter transform from PyTorch, has sub-parameters brightness , contrast , saturation , and hue . General-purpose augmentations Spatial transforms : they only change the resolution (and thereby, the shape) of the input image, and only apply interpolation to the intensities for consistency affine : applies a random affine transformation to the input image; for details, see this page ; has sub-parameters scales (defining the scaling ranges), degrees (defining the rotation ranges), and translation (defining the translation ranges in real-world coordinates , which is usually in mm ) elastic : applies a random elastic deformation to the input image; for details, see this page ; has sub-parameters num_control_points (defining the number of control points), locked_borders (defining the number of locked borders), max_displacement (defining the maximum displacement of the control points), num_control_points (defining the number of control points), and locked_borders (defining the number of locked borders). flip : applies a random flip to the input image; for details, see this page ; has sub-parameter axes (defining the axes to flip). rotate : applies a random rotation by 90 degrees ( rotate_90 ) or 180 degrees ( rotate_180 ), has sub-parameter axes (defining the axes to rotate). swap : applies a random swap , has sub-parameter patch_size (defining the patch size to swap), and num_iterations (number of iterations that 2 patches will be swapped). Intensity transforms : they change the intensity of the input image, but never the actual resolution or shape. motion : applies a random motion blur to the input image using this function . blur : applies a random Gaussian blur to the input image using this function l has sub-parameter std (defines the standard deviation range). noise : applies a random noise to the input image using this function ; has sub-parameters std (defines the standard deviation range) and mean (defines the mean of the noise to be added). noise_var : applies a random noise to the input image, however, the with default std = [0, 0.015 * std(image)] . anisotropic : applies random anisotropic transform to input image using this function . This changes the resolution and brings it back to its original resolution, thus applying \"real-world\" interpolation to images.","title":"Data Augmentation"},{"location":"customize/#training-parameters","text":"These are various parameters that control the overall training process. verbose : generate verbose messages on console; generally used for debugging. batch_size : batch size to be used for training. in_memory : this is to enable or disable lazy loading. If set to True , all data is loaded onto the RAM at once during the construction of the dataloader (either training/validation/testing), thus resulting in faster training. If set to False , data gets read into RAM on-the-go when needed (also called \"lazy loading\" ), which slows down training but lessens the memory load. The latter is recommended if the user's RAM has limited capacity. num_epochs : number of epochs to train for. patience : number of epochs to wait for improvement in the validation loss before early stopping. learning_rate : learning rate to be used for training. scheduler : learning rate scheduler to be used for training, more details are here ; can take the following sub-parameters: type : triangle , triangle_modified , exp , step , reduce-on-plateau , cosineannealing , triangular , triangular2 , exp_range min_lr : minimum learning rate to be used for training. max_lr : maximum learning rate to be used for training. optimizer : optimizer to be used for training, more details are here . nested_training : number of folds to use nested training, takes testing and validation as sub-parameters, with integer values defining the number of folds to use. memory_save_mode : if enabled, resize/resample operations in data_preprocessing will save files to disk instead of directly getting read into memory as tensors Queue configuration : this defines how the queue for the input to the model is to be designed after the patching strategy has been applied, and more details are here . This takes the following sub-parameters: q_max_length : his determines the maximum number of patches that can be stored in the queue. Using a large number means that the queue needs to be filled less often, but more CPU memory is needed to store the patches. q_samples_per_volume : this determines the number of patches to extract from each volume. A small number of patches ensures a large variability in the queue, but training will be slower. q_num_workers : this determines the number subprocesses to use for data loading; '0' means main process is used, scale this according to available CPU resources. q_verbose : used to debug the queue","title":"Training Parameters"},{"location":"customize/#differentially-private-training","text":"GaNDLF supports training differentially private models using Opacus . Here are some resources using which one can train private models: TLDR on DP and private training: read this paper and this blog post . All options are present in a new key called differential_privacy in the config file. It has the following options: noise_multiplier : The ratio of the standard deviation of the Gaussian noise to the L2-sensitivity of the function to which the noise is added. max_grad_norm : The maximum norm of the per-sample gradients. Any gradient with norm higher than this will be clipped to this value. accountant : Accounting mechanism. Currently supported: rdp (RDPAccountant), gdp (GaussianAccountant), prv (PRVAccountant) secure_mode : Set to True if cryptographically strong DP guarantee is required. secure_mode=True uses secure random number generator for noise and shuffling (as opposed to pseudo-rng in vanilla PyTorch) and prevents certain floating-point arithmetic-based attacks. allow_opacus_model_fix : Enabled automated fixing of the model based on Opacus [ ref ] delta : Target delta to be achieved. Probability of information being leaked. Use either this or epsilon . epsilon : Target epsilon to be achieved, a metric of privacy loss at differential changes in data. Use either this or delta . physical_batch_size : The batch size to use for DP computation (it is usually set lower than the baseline or non-DP batch size). Defaults to batch_size .","title":"Differentially Private Training"},{"location":"extending/","text":"Environment \u00b6 Before starting to work on the code-level on GaNDLF, please follow the instructions to install GaNDLF from sources . Once that's done, please verify the installation using the following command: # continue from previous shell ( venv_gandlf ) $> # you should be in the \"GaNDLF\" git repo ( venv_gandlf ) $> gandlf verify-install Submodule flowcharts \u00b6 The following flowcharts are intended to provide a high-level overview of the different submodules in GaNDLF. Navigate to the README.md file in each submodule folder for details. Overall Architecture \u00b6 Command-line parsing: gandlf run Parameters from training configuration get passed as a dict via the config manager Training Manager : Handles k-fold training Main entry point from CLI Training Function : Performs actual training Inference Manager : Handles inference functionality Main entry point from CLI Inference Function : Performs actual inference Dependency Management \u00b6 To update/change/add a dependency in setup , please ensure at least the following conditions are met: The package is being actively maintained . The new dependency is being testing against the minimum python version supported by GaNDLF (see the python_requires variable in setup ). It does not clash with any existing dependencies. Adding Models \u00b6 For details, please see README for GANDLF.models submodule . Update Tests Adding Augmentation Transformations \u00b6 Update or add dependency in setup , if appropriate. Add transformation to global_augs_dict , defined in GANDLF/data/augmentation/__init__.py Ensure probability is used as input; probability is not used for any preprocessing operations For details, please see README for GANDLF.data.augmentation submodule . Update Tests Adding Preprocessing functionality \u00b6 Update or add dependency in setup , if appropriate; see section on Dependency Management for details. All transforms should be defined by inheriting from torchio.transforms.intensity_transform.IntensityTransform . For example, please see the threshold/clip functionality in the GANDLF/data/preprocessing/threshold_and_clip.py file. Define each option in the configuration file under the correct key (again, see threshold/clip as examples) Add transformation to global_preprocessing_dict , defined in GANDLF/data/preprocessing/__init__.py For details, please see README for GANDLF.data.preprocessing submodule . Update Tests Adding Training Functionality \u00b6 Update Training Function Update Training Manager , if any training API has changed Update Tests Adding Inference Functionality \u00b6 Update Inference Function Update Inference Manager , if any inference API has changed Update Tests Adding new CLI command \u00b6 Example: gandlf config-generator CLI command - Implement function and wrap it with @click.command() + @click.option() - Add it to cli_subcommands dict The command would be available under gandlf your-subcommand-name CLI command. Update parameters \u00b6 For any new feature, please ensure the corresponding option in the sample configuration is added, so that others can review/use/extend it as needed. Update Tests \u00b6 Once you have made changes to functionality, it is imperative that the unit tests be updated to cover the new code. Please see the full testing suite for details and examples. Run Tests \u00b6 Prerequisites \u00b6 There are two types of tests: unit tests for GaNDLF code, which tests the functionality, and integration tests for deploying and running mlcubes. Some additional steps are required for running tests: Ensure that the install optional dependencies [ ref ] have been installed. Tests are using sample data , which gets downloaded and prepared automatically when you run unit tests. Prepared data is stored at ${GaNDLF_root_dir}/testing/data/ folder. However, you may want to download & explore data by yourself. Unit tests \u00b6 Once you have the virtual environment set up, tests can be run using the following command: # continue from previous shell ( venv_gandlf ) $> pytest --device cuda # can be cuda or cpu, defaults to cpu Any failures will be reported in the file ${GANDLF_HOME}/testing/failures.log . Integration tests \u00b6 All integration tests are combined to one shell script: # it's assumed you are in `GaNDLF/` repo root directory cd testing/ ./test_deploy.sh Code coverage \u00b6 The code coverage for the unit tests can be obtained by the following command: bash # continue from previous shell ( venv_gandlf ) $> coverage run -m pytest - -device cuda ; coverage report -m Logging \u00b6 Use loggers instead of print \u00b6 We use the native logging library for logs management. This gets automatically configured when GaNDLF gets launched. So, if you are extending the code, please use loggers instead of prints. Here is an example how root logger can be used def my_new_cool_function(df: pd.DataFrame): logging.debug(\"Message for debug file only\") logging.info(\"Hi GaNDLF user, I greet you in the CLI output\") logging.error(f\"A detailed message about any error if needed. Exception: {str(e)}, params: {params}, df shape: {df.shape}\") # do NOT use normal print statements # print(\"Hi GaNDLF user!\") Here is an example how logger can be used: def my_new_cool_function(df: pd.DataFrame): logger = logging.getLogger(__name__) # you can use any your own logger name or just pass a current file name logger.debug(\"Message for debug file only\") logger.info(\"Hi GaNDLF user, I greet you in the CLI output\") logger.error(f\"A detailed message about any error if needed. Exception: {str(e)}, params: {params}, df shape: {df.shape}\") # print(\"Hi GaNDLF user!\") # don't use prints please. What and where is logged \u00b6 GaNDLF logs are splitted into multiple parts: - CLI output: only info messages are shown here - debug file: all messages are shown - stderr: display warning , error , or critical messages By default, the logs are saved in the /tmp/.gandlf dir. The logs are saved in the path that is defined by the '--log-file' parameter in the CLI commands. Example of log message #format: \"%(asctime)s - %(name)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s\" 2024-07-03 13:05:51,642 - root - DEBUG - GaNDLF/GANDLF/entrypoints/anonymizer.py:28 - input_dir='.' Create your own logger \u00b6 You can create and configure your own logger by updating the file GANDLF/logging_config.yaml .","title":"Extending GaNDLF"},{"location":"extending/#environment","text":"Before starting to work on the code-level on GaNDLF, please follow the instructions to install GaNDLF from sources . Once that's done, please verify the installation using the following command: # continue from previous shell ( venv_gandlf ) $> # you should be in the \"GaNDLF\" git repo ( venv_gandlf ) $> gandlf verify-install","title":"Environment"},{"location":"extending/#submodule-flowcharts","text":"The following flowcharts are intended to provide a high-level overview of the different submodules in GaNDLF. Navigate to the README.md file in each submodule folder for details.","title":"Submodule flowcharts"},{"location":"extending/#overall-architecture","text":"Command-line parsing: gandlf run Parameters from training configuration get passed as a dict via the config manager Training Manager : Handles k-fold training Main entry point from CLI Training Function : Performs actual training Inference Manager : Handles inference functionality Main entry point from CLI Inference Function : Performs actual inference","title":"Overall Architecture"},{"location":"extending/#dependency-management","text":"To update/change/add a dependency in setup , please ensure at least the following conditions are met: The package is being actively maintained . The new dependency is being testing against the minimum python version supported by GaNDLF (see the python_requires variable in setup ). It does not clash with any existing dependencies.","title":"Dependency Management"},{"location":"extending/#adding-models","text":"For details, please see README for GANDLF.models submodule . Update Tests","title":"Adding Models"},{"location":"extending/#adding-augmentation-transformations","text":"Update or add dependency in setup , if appropriate. Add transformation to global_augs_dict , defined in GANDLF/data/augmentation/__init__.py Ensure probability is used as input; probability is not used for any preprocessing operations For details, please see README for GANDLF.data.augmentation submodule . Update Tests","title":"Adding Augmentation Transformations"},{"location":"extending/#adding-preprocessing-functionality","text":"Update or add dependency in setup , if appropriate; see section on Dependency Management for details. All transforms should be defined by inheriting from torchio.transforms.intensity_transform.IntensityTransform . For example, please see the threshold/clip functionality in the GANDLF/data/preprocessing/threshold_and_clip.py file. Define each option in the configuration file under the correct key (again, see threshold/clip as examples) Add transformation to global_preprocessing_dict , defined in GANDLF/data/preprocessing/__init__.py For details, please see README for GANDLF.data.preprocessing submodule . Update Tests","title":"Adding Preprocessing functionality"},{"location":"extending/#adding-training-functionality","text":"Update Training Function Update Training Manager , if any training API has changed Update Tests","title":"Adding Training Functionality"},{"location":"extending/#adding-inference-functionality","text":"Update Inference Function Update Inference Manager , if any inference API has changed Update Tests","title":"Adding Inference Functionality"},{"location":"extending/#adding-new-cli-command","text":"Example: gandlf config-generator CLI command - Implement function and wrap it with @click.command() + @click.option() - Add it to cli_subcommands dict The command would be available under gandlf your-subcommand-name CLI command.","title":"Adding new CLI command"},{"location":"extending/#update-parameters","text":"For any new feature, please ensure the corresponding option in the sample configuration is added, so that others can review/use/extend it as needed.","title":"Update parameters"},{"location":"extending/#update-tests","text":"Once you have made changes to functionality, it is imperative that the unit tests be updated to cover the new code. Please see the full testing suite for details and examples.","title":"Update Tests"},{"location":"extending/#run-tests","text":"","title":"Run Tests"},{"location":"extending/#prerequisites","text":"There are two types of tests: unit tests for GaNDLF code, which tests the functionality, and integration tests for deploying and running mlcubes. Some additional steps are required for running tests: Ensure that the install optional dependencies [ ref ] have been installed. Tests are using sample data , which gets downloaded and prepared automatically when you run unit tests. Prepared data is stored at ${GaNDLF_root_dir}/testing/data/ folder. However, you may want to download & explore data by yourself.","title":"Prerequisites"},{"location":"extending/#unit-tests","text":"Once you have the virtual environment set up, tests can be run using the following command: # continue from previous shell ( venv_gandlf ) $> pytest --device cuda # can be cuda or cpu, defaults to cpu Any failures will be reported in the file ${GANDLF_HOME}/testing/failures.log .","title":"Unit tests"},{"location":"extending/#integration-tests","text":"All integration tests are combined to one shell script: # it's assumed you are in `GaNDLF/` repo root directory cd testing/ ./test_deploy.sh","title":"Integration tests"},{"location":"extending/#code-coverage","text":"The code coverage for the unit tests can be obtained by the following command: bash # continue from previous shell ( venv_gandlf ) $> coverage run -m pytest - -device cuda ; coverage report -m","title":"Code coverage"},{"location":"extending/#logging","text":"","title":"Logging"},{"location":"extending/#use-loggers-instead-of-print","text":"We use the native logging library for logs management. This gets automatically configured when GaNDLF gets launched. So, if you are extending the code, please use loggers instead of prints. Here is an example how root logger can be used def my_new_cool_function(df: pd.DataFrame): logging.debug(\"Message for debug file only\") logging.info(\"Hi GaNDLF user, I greet you in the CLI output\") logging.error(f\"A detailed message about any error if needed. Exception: {str(e)}, params: {params}, df shape: {df.shape}\") # do NOT use normal print statements # print(\"Hi GaNDLF user!\") Here is an example how logger can be used: def my_new_cool_function(df: pd.DataFrame): logger = logging.getLogger(__name__) # you can use any your own logger name or just pass a current file name logger.debug(\"Message for debug file only\") logger.info(\"Hi GaNDLF user, I greet you in the CLI output\") logger.error(f\"A detailed message about any error if needed. Exception: {str(e)}, params: {params}, df shape: {df.shape}\") # print(\"Hi GaNDLF user!\") # don't use prints please.","title":"Use loggers instead of print"},{"location":"extending/#what-and-where-is-logged","text":"GaNDLF logs are splitted into multiple parts: - CLI output: only info messages are shown here - debug file: all messages are shown - stderr: display warning , error , or critical messages By default, the logs are saved in the /tmp/.gandlf dir. The logs are saved in the path that is defined by the '--log-file' parameter in the CLI commands. Example of log message #format: \"%(asctime)s - %(name)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s\" 2024-07-03 13:05:51,642 - root - DEBUG - GaNDLF/GANDLF/entrypoints/anonymizer.py:28 - input_dir='.'","title":"What and where is logged"},{"location":"extending/#create-your-own-logger","text":"You can create and configure your own logger by updating the file GANDLF/logging_config.yaml .","title":"Create your own logger"},{"location":"faq/","text":"This page contains answers to frequently asked questions about GaNDLF. Where do I start? \u00b6 The usage guide provides a good starting point for you to understand the application of GaNDLF. If you have any questions, please feel free to post a support request , and we will do our best to address it ASAP. Why do I get the error importlib.metadata.PackageNotFoundError: GANDLF ? \u00b6 This means that GaNDLF was not installed correctly. Please ensure you have followed the installation guide properly. Why is GaNDLF not working? \u00b6 Verify that the installation has been done correctly by running gandlf verify-install after activating the correct virtual environment. If you are still having issues, please feel free to post a support request , and we will do our best to address it ASAP. Which parts of a GaNDLF configuration are customizable? \u00b6 Virtually all of it! For more details, please see the usage guide and our extensive samples . All available options are documented in the config_all_options.yaml file . Can I run GaNDLF on a high performance computing (HPC) cluster? \u00b6 Yes, GaNDLF has successfully been run on an SGE cluster and another managed using Kubernetes. Please post a question with more details such as the type of scheduler, and so on, and we will do our best to address it. How can I track the per-epoch training performance? \u00b6 Yes, look for logs_*.csv files in the output directory. It should be arranged in accordance with the cross-validation configuration. Furthermore, it should contain separate files for each data cohort, i.e., training/validation/testing, along with the values for all requested performance metrics, which are defined per problem type. Why are my compute jobs failing with excess RAM usage? \u00b6 If you have data_preprocessing enabled, GaNDLF will load all of the resized images as tensors into memory. Depending on your dataset (resolution, size, number of modalities), this can lead to high RAM usage. To avoid this, you can enable the memory saver mode by enabling the flag memory_save_mode in the configuration. This will write the resized images into disk. How can I resume training from a previous checkpoint? \u00b6 GaNDLF allows you to resume training from a previous checkpoint in 2 ways: - By using the --resume CLI parameter in gandlf run , only the model weights and state dictionary will be preserved, but parameters and data are taken from the new options in the CLI. This is helpful when you are updated the training data or some compatible options in the parameters. If both --resume and --reset are skipped in gandlf run , the model weights, state dictionary, and all previously saved information (parameters, training/validation/testing data) is used to resume training. How can I update GaNDLF? \u00b6 If you have installed from pip , then you can simply run pip install --upgrade gandlf to get the latest version of GaNDLF, or if you are interested in the nightly builds, then you can run pip install --upgrade --pre gandlf . If you have performed installation from sources , then you will need to do git pull from the base GaNDLF directory to get the latest master of GaNDLF. Follow this up with pip install -e . after activating the appropriate virtual environment to ensure the updates get passed through. How can I perform federated learning of my GaNDLF model? \u00b6 Please see https://mlcommons.github.io/GaNDLF/usage/#federating-your-model-using-openfl. How can I perform federated evaluation of my GaNDLF model? \u00b6 Please see https://mlcommons.github.io/GaNDLF/usage/#federating-your-model-evaluation-using-medperf. I was using GaNDLF version 0.0.19 or earlier, and I am facing issues after updating to 0.0.20 or later. What should I do? \u00b6 Please read the migration guide to understand the changes that have been made to GaNDLF. If you have any questions, please feel free to post a support request . I am getting an error related to version mismatch (greater or smaller) between the configuration and GaNDLF version. What should I do? \u00b6 This is a safety feature to ensure a tight integration between the configuration used to define a model and the code version used to perform the training. Ensure that you have all requirements satisfied, and then check the version key in the configuration, and ensure it appropriately matches the output of gandlf run --version . How to interpret seeing the same numbers for all classification metrics for under global_* ? \u00b6 The classification metrics are based on TorchMetrics [ ref ], and this is an issue that is documented on their side [ ref ]. Please use either per_class_weighted or per_class_average metrics for final evaluation. What if I have another question? \u00b6 Please post a support request .","title":"FAQ"},{"location":"faq/#where-do-i-start","text":"The usage guide provides a good starting point for you to understand the application of GaNDLF. If you have any questions, please feel free to post a support request , and we will do our best to address it ASAP.","title":"Where do I start?"},{"location":"faq/#why-do-i-get-the-error-importlibmetadatapackagenotfounderror-gandlf","text":"This means that GaNDLF was not installed correctly. Please ensure you have followed the installation guide properly.","title":"Why do I get the error importlib.metadata.PackageNotFoundError: GANDLF?"},{"location":"faq/#why-is-gandlf-not-working","text":"Verify that the installation has been done correctly by running gandlf verify-install after activating the correct virtual environment. If you are still having issues, please feel free to post a support request , and we will do our best to address it ASAP.","title":"Why is GaNDLF not working?"},{"location":"faq/#which-parts-of-a-gandlf-configuration-are-customizable","text":"Virtually all of it! For more details, please see the usage guide and our extensive samples . All available options are documented in the config_all_options.yaml file .","title":"Which parts of a GaNDLF configuration are customizable?"},{"location":"faq/#can-i-run-gandlf-on-a-high-performance-computing-hpc-cluster","text":"Yes, GaNDLF has successfully been run on an SGE cluster and another managed using Kubernetes. Please post a question with more details such as the type of scheduler, and so on, and we will do our best to address it.","title":"Can I run GaNDLF on a high performance computing (HPC) cluster?"},{"location":"faq/#how-can-i-track-the-per-epoch-training-performance","text":"Yes, look for logs_*.csv files in the output directory. It should be arranged in accordance with the cross-validation configuration. Furthermore, it should contain separate files for each data cohort, i.e., training/validation/testing, along with the values for all requested performance metrics, which are defined per problem type.","title":"How can I track the per-epoch training performance?"},{"location":"faq/#why-are-my-compute-jobs-failing-with-excess-ram-usage","text":"If you have data_preprocessing enabled, GaNDLF will load all of the resized images as tensors into memory. Depending on your dataset (resolution, size, number of modalities), this can lead to high RAM usage. To avoid this, you can enable the memory saver mode by enabling the flag memory_save_mode in the configuration. This will write the resized images into disk.","title":"Why are my compute jobs failing with excess RAM usage?"},{"location":"faq/#how-can-i-resume-training-from-a-previous-checkpoint","text":"GaNDLF allows you to resume training from a previous checkpoint in 2 ways: - By using the --resume CLI parameter in gandlf run , only the model weights and state dictionary will be preserved, but parameters and data are taken from the new options in the CLI. This is helpful when you are updated the training data or some compatible options in the parameters. If both --resume and --reset are skipped in gandlf run , the model weights, state dictionary, and all previously saved information (parameters, training/validation/testing data) is used to resume training.","title":"How can I resume training from a previous checkpoint?"},{"location":"faq/#how-can-i-update-gandlf","text":"If you have installed from pip , then you can simply run pip install --upgrade gandlf to get the latest version of GaNDLF, or if you are interested in the nightly builds, then you can run pip install --upgrade --pre gandlf . If you have performed installation from sources , then you will need to do git pull from the base GaNDLF directory to get the latest master of GaNDLF. Follow this up with pip install -e . after activating the appropriate virtual environment to ensure the updates get passed through.","title":"How can I update GaNDLF?"},{"location":"faq/#how-can-i-perform-federated-learning-of-my-gandlf-model","text":"Please see https://mlcommons.github.io/GaNDLF/usage/#federating-your-model-using-openfl.","title":"How can I perform federated learning of my GaNDLF model?"},{"location":"faq/#how-can-i-perform-federated-evaluation-of-my-gandlf-model","text":"Please see https://mlcommons.github.io/GaNDLF/usage/#federating-your-model-evaluation-using-medperf.","title":"How can I perform federated evaluation of my GaNDLF model?"},{"location":"faq/#i-was-using-gandlf-version-0019-or-earlier-and-i-am-facing-issues-after-updating-to-0020-or-later-what-should-i-do","text":"Please read the migration guide to understand the changes that have been made to GaNDLF. If you have any questions, please feel free to post a support request .","title":"I was using GaNDLF version 0.0.19 or earlier, and I am facing issues after updating to 0.0.20 or later. What should I do?"},{"location":"faq/#i-am-getting-an-error-related-to-version-mismatch-greater-or-smaller-between-the-configuration-and-gandlf-version-what-should-i-do","text":"This is a safety feature to ensure a tight integration between the configuration used to define a model and the code version used to perform the training. Ensure that you have all requirements satisfied, and then check the version key in the configuration, and ensure it appropriately matches the output of gandlf run --version .","title":"I am getting an error related to version mismatch (greater or smaller) between the configuration and GaNDLF version. What should I do?"},{"location":"faq/#how-to-interpret-seeing-the-same-numbers-for-all-classification-metrics-for-under-global_","text":"The classification metrics are based on TorchMetrics [ ref ], and this is an issue that is documented on their side [ ref ]. Please use either per_class_weighted or per_class_average metrics for final evaluation.","title":"How to interpret seeing the same numbers for all classification metrics for under global_*?"},{"location":"faq/#what-if-i-have-another-question","text":"Please post a support request .","title":"What if I have another question?"},{"location":"getting_started/","text":"This document will help you get started with GaNDLF using a few representative examples. Installation \u00b6 Follow the installation instructions to install GaNDLF. When the installation is complete, you should end up with the following shell, which indicates that the GaNDLF virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here Running GaNDLF with GitHub Codespaces \u00b6 Alternatively, you can launch a Codespace for GaNDLF by clicking this link: A codespace will open in a web-based version of Visual Studio Code . The dev container is fully configured with software needed for this project. Note : Dev Containers is an open spec which is supported by GitHub Codespaces and other tools . Sample Data \u00b6 Sample data will be used for our extensive automated unit tests in all examples. You can download the sample data from this link . An example is shown below: # continue from previous shell ( venv_gandlf ) $> wget https://upenn.box.com/shared/static/y8162xkq1zz5555ye3pwadry2m2e39bs.zip -O ./gandlf_sample_data.zip ( venv_gandlf ) $> unzip ./gandlf_sample_data.zip # this should extract a directory called `data` in the current directory The data directory content should look like the example below (for brevity, these locations shall be referred to as ${GANDLF_DATA} in the rest of the document): # continue from previous shell ( venv_gandlf ) $> ls data 2d_histo_segmentation 2d_rad_segmentation 3d_rad_segmentation # and a bunch of CSVs which can be ignored Note : When using your own data, it is vital to correctly prepare your data prior to using it for any computational task (such as AI training or inference). Segmentation \u00b6 Segmentation using 3D Radiology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. Furthermore, the CSV should look like the example below (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,Label 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,${GANDLF_DATA_3DRAD}/001/mask.nii.gz 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,${GANDLF_DATA_3DRAD}/002/mask.nii.gz 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,${GANDLF_DATA_3DRAD}/003/mask.nii.gz 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,${GANDLF_DATA_3DRAD}/004/mask.nii.gz 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,${GANDLF_DATA_3DRAD}/005/mask.nii.gz 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,${GANDLF_DATA_3DRAD}/006/mask.nii.gz 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,${GANDLF_DATA_3DRAD}/007/mask.nii.gz 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,${GANDLF_DATA_3DRAD}/008/mask.nii.gz 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,${GANDLF_DATA_3DRAD}/009/mask.nii.gz 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,${GANDLF_DATA_3DRAD}/010/mask.nii.gz 3. Construct the configuration file to help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, just without Label or ValueToPredict headers. Segmentation using 2D Histology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,Label 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_720-3344_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_816-3488_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_960-3376_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_976-3520_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1024-3216_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1104-3360_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1168-3104_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1248-3248_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1312-3056_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1392-3200_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_720-3344_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_816-3488_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_960-3376_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_976-3520_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1024-3216_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1104-3360_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1168-3104_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1248-3248_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1312-3056_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1392-3200_LM.png 5. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 6. Now you are ready to train your model . 7. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Note : Please consider the special considerations for histology images during inference . Classification \u00b6 Classification using 3D Radiology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. The CSV should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,0 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,1 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,0 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,2 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,0 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,1 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,0 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,2 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,0 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,1 3. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Classification (patch-level) using 2D Histology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,1 5. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 6. Now you are ready to train your model . 7. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Note : Please consider the special considerations for histology images during inference . Regression \u00b6 Regression using 3D Radiology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. The CSV should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,0.4 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,1.2 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,0.2 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,2.3 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,0.4 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,1.2 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,0.3 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,2.2 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,0.1 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,1.5 3. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Regression (patch-level) using 2D Histology Images \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,0.1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,0.2 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,0.3 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,0.6 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,1.5 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,1.3 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,1.0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,1.5 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,1.1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,0.4 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,0.5 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,0.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,0.5 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,0.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,1.3 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,1.1 4. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 5. Now you are ready to train your model . 6. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. 7. Note : Please consider the special considerations for histology images during inference .","title":"Getting Started"},{"location":"getting_started/#installation","text":"Follow the installation instructions to install GaNDLF. When the installation is complete, you should end up with the following shell, which indicates that the GaNDLF virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here","title":"Installation"},{"location":"getting_started/#running-gandlf-with-github-codespaces","text":"Alternatively, you can launch a Codespace for GaNDLF by clicking this link: A codespace will open in a web-based version of Visual Studio Code . The dev container is fully configured with software needed for this project. Note : Dev Containers is an open spec which is supported by GitHub Codespaces and other tools .","title":"Running GaNDLF with GitHub Codespaces"},{"location":"getting_started/#sample-data","text":"Sample data will be used for our extensive automated unit tests in all examples. You can download the sample data from this link . An example is shown below: # continue from previous shell ( venv_gandlf ) $> wget https://upenn.box.com/shared/static/y8162xkq1zz5555ye3pwadry2m2e39bs.zip -O ./gandlf_sample_data.zip ( venv_gandlf ) $> unzip ./gandlf_sample_data.zip # this should extract a directory called `data` in the current directory The data directory content should look like the example below (for brevity, these locations shall be referred to as ${GANDLF_DATA} in the rest of the document): # continue from previous shell ( venv_gandlf ) $> ls data 2d_histo_segmentation 2d_rad_segmentation 3d_rad_segmentation # and a bunch of CSVs which can be ignored Note : When using your own data, it is vital to correctly prepare your data prior to using it for any computational task (such as AI training or inference).","title":"Sample Data"},{"location":"getting_started/#segmentation","text":"","title":"Segmentation"},{"location":"getting_started/#segmentation-using-3d-radiology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. Furthermore, the CSV should look like the example below (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,Label 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,${GANDLF_DATA_3DRAD}/001/mask.nii.gz 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,${GANDLF_DATA_3DRAD}/002/mask.nii.gz 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,${GANDLF_DATA_3DRAD}/003/mask.nii.gz 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,${GANDLF_DATA_3DRAD}/004/mask.nii.gz 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,${GANDLF_DATA_3DRAD}/005/mask.nii.gz 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,${GANDLF_DATA_3DRAD}/006/mask.nii.gz 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,${GANDLF_DATA_3DRAD}/007/mask.nii.gz 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,${GANDLF_DATA_3DRAD}/008/mask.nii.gz 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,${GANDLF_DATA_3DRAD}/009/mask.nii.gz 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,${GANDLF_DATA_3DRAD}/010/mask.nii.gz 3. Construct the configuration file to help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, just without Label or ValueToPredict headers.","title":"Segmentation using 3D Radiology Images"},{"location":"getting_started/#segmentation-using-2d-histology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,Label 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_720-3344_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_816-3488_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_960-3376_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_976-3520_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1024-3216_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1104-3360_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1168-3104_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1248-3248_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1312-3056_LM.png 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,${GANDLF_DATA_HISTO_PATCHES}/1/mask/mask_patch_1392-3200_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_720-3344_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_816-3488_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_960-3376_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_976-3520_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1024-3216_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1104-3360_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1168-3104_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1248-3248_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1312-3056_LM.png 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,${GANDLF_DATA_HISTO_PATCHES}/2/mask/mask_patch_1392-3200_LM.png 5. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 6. Now you are ready to train your model . 7. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Note : Please consider the special considerations for histology images during inference .","title":"Segmentation using 2D Histology Images"},{"location":"getting_started/#classification","text":"","title":"Classification"},{"location":"getting_started/#classification-using-3d-radiology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. The CSV should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,0 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,1 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,0 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,2 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,0 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,1 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,0 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,2 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,0 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,1 3. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers.","title":"Classification using 3D Radiology Images"},{"location":"getting_started/#classification-patch-level-using-2d-histology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,0 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,1 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,1 5. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 6. Now you are ready to train your model . 7. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. Note : Please consider the special considerations for histology images during inference .","title":"Classification (patch-level) using 2D Histology Images"},{"location":"getting_started/#regression","text":"","title":"Regression"},{"location":"getting_started/#regression-using-3d-radiology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sample data for this task, the base location is ${GANDLF_DATA}/3d_rad_segmentation , and it will be referred to as ${GANDLF_DATA_3DRAD} in the rest of the document. The CSV should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 001,${GANDLF_DATA_3DRAD}/001/image.nii.gz,0.4 002,${GANDLF_DATA_3DRAD}/002/image.nii.gz,1.2 003,${GANDLF_DATA_3DRAD}/003/image.nii.gz,0.2 004,${GANDLF_DATA_3DRAD}/004/image.nii.gz,2.3 005,${GANDLF_DATA_3DRAD}/005/image.nii.gz,0.4 006,${GANDLF_DATA_3DRAD}/006/image.nii.gz,1.2 007,${GANDLF_DATA_3DRAD}/007/image.nii.gz,0.3 008,${GANDLF_DATA_3DRAD}/008/image.nii.gz,2.2 009,${GANDLF_DATA_3DRAD}/009/image.nii.gz,0.1 010,${GANDLF_DATA_3DRAD}/010/image.nii.gz,1.5 3. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 4. Now you are ready to train your model . 5. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers.","title":"Regression using 3D Radiology Images"},{"location":"getting_started/#regression-patch-level-using-2d-histology-images","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Extract patches/tiles from the full-size whole slide images for training. A sample configuration to extract patches is presented here : num_patches : 3 patch_size : - 1000m - 1000m Assuming the output will be stored in ${GANDLF_DATA}/histo_patches_output , you can refer to this location as ${GANDLF_DATA_HISTO_PATCHES} in the rest of the document. Construct the main data file that will be used for the entire computation cycle. The sample data for this task should be generated after the patches are extracted . It should look like the following example (currently, the Label header is unused and ignored for classification/regression, which use the ValueToPredict header): SubjectID,Channel_0,ValueToPredict 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_720-3344.png,0.1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_816-3488.png,0.2 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_960-3376.png,0.3 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_976-3520.png,0.6 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1024-3216.png,1.5 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1104-3360.png,1.3 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1168-3104.png,1.0 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1248-3248.png,1.5 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1312-3056.png,1.1 1,${GANDLF_DATA_HISTO_PATCHES}/1/image/image_patch_1392-3200.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_720-3344.png,0.4 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_816-3488.png,0.5 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_960-3376.png,0.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_976-3520.png,0.5 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1024-3216.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1104-3360.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1168-3104.png,0.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1248-3248.png,1.3 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1312-3056.png,1.2 2,${GANDLF_DATA_HISTO_PATCHES}/2/image/image_patch_1392-3200.png,1.1 4. Construct the configuration file that will help design the computation (training and inference) pipeline. An example file for this task can be found here . This configuration has various levels of customization, and those details are presented on this page . 5. Now you are ready to train your model . 6. Once the model is trained, you can infer it on unseen data. Remember to construct a similar data file for the unseen data, but without Label or ValueToPredict headers. 7. Note : Please consider the special considerations for histology images during inference .","title":"Regression (patch-level) using 2D Histology Images"},{"location":"itcr_connectivity/","text":"ITCR Connectivity \u00b6 This section includes a reference of all ongoing and existing connections between FeTS and other projects funded under the Informatics Technology for Cancer Research (ITCR) program. A connectivity map featuring all ITCR projects can be found here . Existing Connections DCMTK Synapse PACS FLAIM Ongoing Development XNAT OHIF Radiomics.io RadXTools TCIA Existing Connections \u00b6 DCMTK \u00b6 GaNDLF uses DCMTK - DICOM ToolKit (DCMTK) (through ITK ) for DICOM file handling. Synapse PACS \u00b6 GaNDLF's Metrics Suite is used by Synapse PACS . FLAIM \u00b6 GaNDLF models can be ingested through FLAIM to facilitate interoperability and privacy preservation. Ongoing Development \u00b6 XNAT \u00b6 Enable the use of GaNDLF's models to databases and population cohorts defined by XNAT . OHIF \u00b6 Online visualization using OHIF. Radiomics \u00b6 Extraction of downstream features of automatically generated segmentation maps using Radiomics.io . RadXTools \u00b6 Integration with RadXTools for extended functionality. TCIA \u00b6 Integration with TCIA's REST APIs to make downloading datasets easier. Contact gandlf [at] mlcommons.org with any questions.","title":"ITCR Connectivity"},{"location":"itcr_connectivity/#itcr-connectivity","text":"This section includes a reference of all ongoing and existing connections between FeTS and other projects funded under the Informatics Technology for Cancer Research (ITCR) program. A connectivity map featuring all ITCR projects can be found here . Existing Connections DCMTK Synapse PACS FLAIM Ongoing Development XNAT OHIF Radiomics.io RadXTools TCIA","title":"ITCR Connectivity"},{"location":"itcr_connectivity/#existing-connections","text":"","title":"Existing Connections"},{"location":"itcr_connectivity/#dcmtk","text":"GaNDLF uses DCMTK - DICOM ToolKit (DCMTK) (through ITK ) for DICOM file handling.","title":"DCMTK"},{"location":"itcr_connectivity/#synapse-pacs","text":"GaNDLF's Metrics Suite is used by Synapse PACS .","title":"Synapse PACS"},{"location":"itcr_connectivity/#flaim","text":"GaNDLF models can be ingested through FLAIM to facilitate interoperability and privacy preservation.","title":"FLAIM"},{"location":"itcr_connectivity/#ongoing-development","text":"","title":"Ongoing Development"},{"location":"itcr_connectivity/#xnat","text":"Enable the use of GaNDLF's models to databases and population cohorts defined by XNAT .","title":"XNAT"},{"location":"itcr_connectivity/#ohif","text":"Online visualization using OHIF.","title":"OHIF"},{"location":"itcr_connectivity/#radiomics","text":"Extraction of downstream features of automatically generated segmentation maps using Radiomics.io .","title":"Radiomics"},{"location":"itcr_connectivity/#radxtools","text":"Integration with RadXTools for extended functionality.","title":"RadXTools"},{"location":"itcr_connectivity/#tcia","text":"Integration with TCIA's REST APIs to make downloading datasets easier. Contact gandlf [at] mlcommons.org with any questions.","title":"TCIA"},{"location":"migration_guide/","text":"Migration Guide \u00b6 Post-0.0.20 Release \u00b6 The 0.0.20 release was the final release that supported the old way of using GaNDLF (i.e., gandlf_run ). Instead, we now have a CLI that is more unified and based on modern CLI parsing (i.e., gandlf run ). If you have been using version 0.0.20 or earlier, please follow this guide to move your experimental setup to the new CLI [ ref ]. User-level Changes \u00b6 Command Line Interfaces \u00b6 The CLI commands have been moved to use click for parsing the command line arguments. This means that the commands are now more user-friendly and easier to remember, as well as with added features like tab completion and type checks. All the commands that were previously available in as gandlf_${functionality} are now available as gandlf ${functionality} (i.e., replace the _ with ). The previous commands are still present, but they are deprecated and will be removed in a future release. Configuration Files \u00b6 The main change is the use of the Version package for systematic semantic versioning [ ref ]. No change is needed if you are using a stable version . If you have installed GaNDLF from source or using a nightly build , you will need to ensure that the maximum key under version in the configuration file contains the correct version number: Either including the -dev identifier of the current version (e.g., if the current version is 0.X.Y-dev , then the maximum key should be 0.X.Y-dev ). Or excluding the -dev identifier of the current version, but increasing the version number by one on any level (e.g., if the current version is 0.X.Y-dev , then the maximum key should be 0.X.Y ). Use in HPC Environments \u00b6 If you are using GaNDLF in an HPC environment, you will need to update the job submission scripts to use the new CLI commands. The previous API required one to call the interpreter and the specific command (e.g., ${venv_gandlf}/bin/python gandlf_run ), while the new API requires one to call the GaNDLF command directly (e.g., ${venv_gandlf}/bin/gandlf run or ${venv_gandlf}/bin/gandlf_run ). The Slurm experiments template has been appropriately updated to reflect this change. Developer-level Changes \u00b6 Command Line Interfaces \u00b6 CLI entrypoints are now defined in the GANDLF.entrypoints module, which contains argument parsing (using both the old and new API structures). CLI entrypoint logic is now defined in the GANDLF.cli module, which only contains how the specific functionality is executed from an algorithmic perspective. This is to ensure backwards API compatibility, and will not be removed. Configuration Files \u00b6 GaNDLF's config_manager module is now the primary way to manage configuration files. This is going to be updated to use pydantic in the near future [ ref ]. Post 0.1.4-dev release \u00b6 The 0.1.4-dev release has introduced Pytorch Lightning as the primary orchestration framework for core GaNDLF functionality. From the user perspective, the changes are minimal, but the underlying architecture has been significantly updated. User-level Changes \u00b6 Command Line Interfaces \u00b6 The need for specifying the used device from CLI command has been deprecated and will be removed in a future releases. The usage of given accelerator is now configured via configuration file. Thus, removal of --device \\ -d argument from the CLI commands is recommended, as it does not have any effect anymore and will result in error in future releases. Configuration Files \u00b6 New parameters are introduced in the configuration file to control distributed training, mixed precision training, and automatic searching for optimal learning rate or batch size. The new parameters are: - accelerator - defaults to auto , you can explicitly set it to cpu , gpu , or tpu . auto will try to use GPU if available, otherwise CPU. - precision - defaults to \"32-true\" , 16 , 32 , 64 , \"64-true\" , \"32-true\" , \"16-mixed\" , \"bf16\" , \"bf16-mixed\" . - n_nodes - defaults to 1 , set this to the number of compute nodes you wish to use. Ensure that the machines are properly interconnected, and that the gandlf run command is executed on all nodes. - devices - defaults to auto , you can explicitly set it to a list of device ids to use. auto will try to use all available devices. Note that you need to launch gandlf run on each node number of times equal to the number of devices you want to use (one process per device). - strategy - defaults to auto , specifies the strategy for distributed training. auto will try to use ddp (Data Distributed Parallel) if multiple GPUs are available. To read more about the strategies and their differences from currently supported Data Parallel strategy, refer to Pytorch documentation - auto_lr_find - defaults to false , if set to true , the learning rate will be automatically determined using the learning rate finder at the beginning of the training. To read more, refer to Pytorch Lightning documentation - auto_batch_size_find - defaults to false , if set to true , the batch size will be automatically determined using the batch size finder at the beginning of the training. The batch size will be set to the highest number possible that fits into the memory of the device. Note that batch size has effect only for the training dataloader, as the validation, test, and inference dataloaders use batch size of 1 due to the nature of the evaluation process implemented in GaNDLF. To read more, refer to Pytorch Lightning documentation - num_workers_dataloader - defaults to 1 , set this to the number of workers to use for the dataloader. This parameter will be used across all dataloaders in the training, validation, test, and inference. Note that the number of workers is limited by the number of CPU cores available on the machine. To read more, refer to Pytorch documentation - pin_memory_dataloader - defaults to false , if set to true , the dataloader will copy the data to the page-locked memory, which can increase the speed of data transfer to the GPU. To read more, refer to Pytorch documentation - prefetch_factor_dataloader - defaults to 2 , set this to the number of batches to prefetch per worker during data loading. To read more, refer to Pytorch documentation - parallel compute command - this command is deprecated and will be removed in future releases. The parallel compute command is now automatically determined based on the accelerator and the number of nodes and devices specified in the configuration file. Use in HPC Environments \u00b6 As mentioned in the previous section about the configuration file, distributed training is now done automatically by Pytorch Lightning. User has to only specify the number of nodes and devices in the configuration file, and ensure proper number of processes in each node is launched. The gandlf run command should be executed on each node number of times equal to the number of devices you want to use (one process per device). Example of script configuration to use when running with SLURM scheduler in the batch mode: ### When configuring SBATCH parameters, pay special attention to the following ones: #SBATCH --nodes=2 # number of nodes #SBATCH --ntasks-per-node=4 # number of tasks per node, should be equal to the number of devices you want to use per node #SBATCH --cpus-per-task=10 # number of CPUs per task, should be equal to the number of workers in the dataloader per launched process #SBATCH --gpus-per-task=1 # assign one GPU per task ### run the gandlf using srun to execute it n times on each node, where n is the number of tasks per node srun gandlf run .... Developer-level Changes \u00b6 Training and inference logic \u00b6 Entire logic of training and inference is now handled by the LightningModule abstraction. The logic has been moved from previous implementation and is controlled by GandlfLightningModule in the GANDLF/models module. Using Lightning enforces implementation of specific method encapsulating the logic of each step of the training process. To read more about the LightningModule, refer to Pytorch Lightning documentation Data loading \u00b6 Entire logic of preparing the dataset objects and configuring the dataloaders is now handled by the GandlfTrainingDatamodule and GandlfInferenceDatamodule in the GANDLF/data module. Some data objects that need to be constructed on the fly (during validation, test or inference) are handled in the previously described GandlfLightningModule , adhering to the previous GaNDLF logic. To read more about the Lightning DataModule, refer to Pytorch Lightning documentation Execution of the training and inference \u00b6 Order of operations, as well as configuration of the compute environment, is now handled by the Trainer class from Lightning. To read more about the Trainer, refer to Pytorch Lightning documentation","title":"Migration Guide"},{"location":"migration_guide/#migration-guide","text":"","title":"Migration Guide"},{"location":"migration_guide/#post-0020-release","text":"The 0.0.20 release was the final release that supported the old way of using GaNDLF (i.e., gandlf_run ). Instead, we now have a CLI that is more unified and based on modern CLI parsing (i.e., gandlf run ). If you have been using version 0.0.20 or earlier, please follow this guide to move your experimental setup to the new CLI [ ref ].","title":"Post-0.0.20 Release"},{"location":"migration_guide/#user-level-changes","text":"","title":"User-level Changes"},{"location":"migration_guide/#command-line-interfaces","text":"The CLI commands have been moved to use click for parsing the command line arguments. This means that the commands are now more user-friendly and easier to remember, as well as with added features like tab completion and type checks. All the commands that were previously available in as gandlf_${functionality} are now available as gandlf ${functionality} (i.e., replace the _ with ). The previous commands are still present, but they are deprecated and will be removed in a future release.","title":"Command Line Interfaces"},{"location":"migration_guide/#configuration-files","text":"The main change is the use of the Version package for systematic semantic versioning [ ref ]. No change is needed if you are using a stable version . If you have installed GaNDLF from source or using a nightly build , you will need to ensure that the maximum key under version in the configuration file contains the correct version number: Either including the -dev identifier of the current version (e.g., if the current version is 0.X.Y-dev , then the maximum key should be 0.X.Y-dev ). Or excluding the -dev identifier of the current version, but increasing the version number by one on any level (e.g., if the current version is 0.X.Y-dev , then the maximum key should be 0.X.Y ).","title":"Configuration Files"},{"location":"migration_guide/#use-in-hpc-environments","text":"If you are using GaNDLF in an HPC environment, you will need to update the job submission scripts to use the new CLI commands. The previous API required one to call the interpreter and the specific command (e.g., ${venv_gandlf}/bin/python gandlf_run ), while the new API requires one to call the GaNDLF command directly (e.g., ${venv_gandlf}/bin/gandlf run or ${venv_gandlf}/bin/gandlf_run ). The Slurm experiments template has been appropriately updated to reflect this change.","title":"Use in HPC Environments"},{"location":"migration_guide/#developer-level-changes","text":"","title":"Developer-level Changes"},{"location":"migration_guide/#command-line-interfaces_1","text":"CLI entrypoints are now defined in the GANDLF.entrypoints module, which contains argument parsing (using both the old and new API structures). CLI entrypoint logic is now defined in the GANDLF.cli module, which only contains how the specific functionality is executed from an algorithmic perspective. This is to ensure backwards API compatibility, and will not be removed.","title":"Command Line Interfaces"},{"location":"migration_guide/#configuration-files_1","text":"GaNDLF's config_manager module is now the primary way to manage configuration files. This is going to be updated to use pydantic in the near future [ ref ].","title":"Configuration Files"},{"location":"migration_guide/#post-014-dev-release","text":"The 0.1.4-dev release has introduced Pytorch Lightning as the primary orchestration framework for core GaNDLF functionality. From the user perspective, the changes are minimal, but the underlying architecture has been significantly updated.","title":"Post 0.1.4-dev release"},{"location":"migration_guide/#user-level-changes_1","text":"","title":"User-level Changes"},{"location":"migration_guide/#command-line-interfaces_2","text":"The need for specifying the used device from CLI command has been deprecated and will be removed in a future releases. The usage of given accelerator is now configured via configuration file. Thus, removal of --device \\ -d argument from the CLI commands is recommended, as it does not have any effect anymore and will result in error in future releases.","title":"Command Line Interfaces"},{"location":"migration_guide/#configuration-files_2","text":"New parameters are introduced in the configuration file to control distributed training, mixed precision training, and automatic searching for optimal learning rate or batch size. The new parameters are: - accelerator - defaults to auto , you can explicitly set it to cpu , gpu , or tpu . auto will try to use GPU if available, otherwise CPU. - precision - defaults to \"32-true\" , 16 , 32 , 64 , \"64-true\" , \"32-true\" , \"16-mixed\" , \"bf16\" , \"bf16-mixed\" . - n_nodes - defaults to 1 , set this to the number of compute nodes you wish to use. Ensure that the machines are properly interconnected, and that the gandlf run command is executed on all nodes. - devices - defaults to auto , you can explicitly set it to a list of device ids to use. auto will try to use all available devices. Note that you need to launch gandlf run on each node number of times equal to the number of devices you want to use (one process per device). - strategy - defaults to auto , specifies the strategy for distributed training. auto will try to use ddp (Data Distributed Parallel) if multiple GPUs are available. To read more about the strategies and their differences from currently supported Data Parallel strategy, refer to Pytorch documentation - auto_lr_find - defaults to false , if set to true , the learning rate will be automatically determined using the learning rate finder at the beginning of the training. To read more, refer to Pytorch Lightning documentation - auto_batch_size_find - defaults to false , if set to true , the batch size will be automatically determined using the batch size finder at the beginning of the training. The batch size will be set to the highest number possible that fits into the memory of the device. Note that batch size has effect only for the training dataloader, as the validation, test, and inference dataloaders use batch size of 1 due to the nature of the evaluation process implemented in GaNDLF. To read more, refer to Pytorch Lightning documentation - num_workers_dataloader - defaults to 1 , set this to the number of workers to use for the dataloader. This parameter will be used across all dataloaders in the training, validation, test, and inference. Note that the number of workers is limited by the number of CPU cores available on the machine. To read more, refer to Pytorch documentation - pin_memory_dataloader - defaults to false , if set to true , the dataloader will copy the data to the page-locked memory, which can increase the speed of data transfer to the GPU. To read more, refer to Pytorch documentation - prefetch_factor_dataloader - defaults to 2 , set this to the number of batches to prefetch per worker during data loading. To read more, refer to Pytorch documentation - parallel compute command - this command is deprecated and will be removed in future releases. The parallel compute command is now automatically determined based on the accelerator and the number of nodes and devices specified in the configuration file.","title":"Configuration Files"},{"location":"migration_guide/#use-in-hpc-environments_1","text":"As mentioned in the previous section about the configuration file, distributed training is now done automatically by Pytorch Lightning. User has to only specify the number of nodes and devices in the configuration file, and ensure proper number of processes in each node is launched. The gandlf run command should be executed on each node number of times equal to the number of devices you want to use (one process per device). Example of script configuration to use when running with SLURM scheduler in the batch mode: ### When configuring SBATCH parameters, pay special attention to the following ones: #SBATCH --nodes=2 # number of nodes #SBATCH --ntasks-per-node=4 # number of tasks per node, should be equal to the number of devices you want to use per node #SBATCH --cpus-per-task=10 # number of CPUs per task, should be equal to the number of workers in the dataloader per launched process #SBATCH --gpus-per-task=1 # assign one GPU per task ### run the gandlf using srun to execute it n times on each node, where n is the number of tasks per node srun gandlf run ....","title":"Use in HPC Environments"},{"location":"migration_guide/#developer-level-changes_1","text":"","title":"Developer-level Changes"},{"location":"migration_guide/#training-and-inference-logic","text":"Entire logic of training and inference is now handled by the LightningModule abstraction. The logic has been moved from previous implementation and is controlled by GandlfLightningModule in the GANDLF/models module. Using Lightning enforces implementation of specific method encapsulating the logic of each step of the training process. To read more about the LightningModule, refer to Pytorch Lightning documentation","title":"Training and inference logic"},{"location":"migration_guide/#data-loading","text":"Entire logic of preparing the dataset objects and configuring the dataloaders is now handled by the GandlfTrainingDatamodule and GandlfInferenceDatamodule in the GANDLF/data module. Some data objects that need to be constructed on the fly (during validation, test or inference) are handled in the previously described GandlfLightningModule , adhering to the previous GaNDLF logic. To read more about the Lightning DataModule, refer to Pytorch Lightning documentation","title":"Data loading"},{"location":"migration_guide/#execution-of-the-training-and-inference","text":"Order of operations, as well as configuration of the compute environment, is now handled by the Trainer class from Lightning. To read more about the Trainer, refer to Pytorch Lightning documentation","title":"Execution of the training and inference"},{"location":"setup/","text":"Setup/Installation Instructions \u00b6 Prerequisites \u00b6 Python3 with a preference for conda , and python version 3.9 (higher versions might work, but they are untested ). Knowledge of managing Python environments . The instructions below assume knowledge of the conda management system . Alternatively, you can run GaNDLF via Docker . This needs different prerequisites. See the Docker Installation section below for more information. Optional Requirements \u00b6 GPU compute (usually needed for faster training): Install appropriate drivers: NVIDIA AMD Compute toolkit appropriate for your hardware: NVIDIA: CUDA and a compatible cuDNN installed system-wide AMD: ROCm Windows: Microsoft Visual C++ 14.0 or greater . This is required for PyTorch to work on Windows. If you are using conda, you can install it using the following command for your virtual environment: conda install -c anaconda m2w64-toolchain . Installation \u00b6 Install PyTorch \u00b6 GaNDLF's primary computational foundation is built on PyTorch, and as such it supports all hardware types that PyTorch supports. Please install PyTorch for your hardware type before installing GaNDLF. See the PyTorch installation instructions for more details. First, instantiate your environment ( base ) $> conda create -n venv_gandlf python = 3 .11 -y ( base ) $> conda activate venv_gandlf ( venv_gandlf ) $> ### subsequent commands go here You may install pytorch to be compatible with CUDA, ROCm, or CPU-only. An exhaustive list of PyTorch installations for the specific version compatible with GaNDLF can be found here: https://pytorch.org/get-started/previous-versions/ Optional Dependencies \u00b6 The following dependencies are optional , and are only needed to access specific features of GaNDLF. ( venv_gandlf ) $> pip install openvino-dev == 2023 .0.1 # [OPTIONAL] to generate post-training optimized models for inference Install from Package Managers \u00b6 This option is recommended for most users, and allows for the quickest way to get started with GaNDLF. ( venv_gandlf ) $> pip install gandlf # this will give you the latest stable release You can also use conda ( venv_gandlf ) $> conda install -c conda-forge gandlf -y If you are interested in running the latest version of GaNDLF, you can install the nightly build by running the following command: ( venv_gandlf ) $> pip install --pre gandlf You can also use conda ( venv_gandlf ) $> conda install -c conda-forge/label/gandlf_dev -c conda-forge gandlf -y Install from Sources \u00b6 Use this option if you want to contribute to GaNDLF , or are interested to make other code-level changes for your own use. ( venv_gandlf ) $> git clone https://github.com/mlcommons/GaNDLF.git ( venv_gandlf ) $> cd GaNDLF ( venv_gandlf ) $> pip install -e . Test your installation: ( venv_gandlf ) $> gandlf verify-install Docker Installation \u00b6 We provide containerized versions of GaNDLF, which allows you to run GaNDLF without worrying about installation steps or dependencies. Steps to run the Docker version of GaNDLF \u00b6 Install the Docker Engine for your platform. GaNDLF is available from GitHub Package Registry . Several platform versions are available, including support for CUDA, ROCm, and CPU-only. Choose the one that best matches your system and drivers. For example, if you want to get the bleeding-edge GaNDLF version, and you have CUDA Toolkit v11.6, run the following command: ( base ) $> docker pull ghcr.io/mlcommons/gandlf:latest-cuda116 This will download the GaNDLF image onto your machine. See the usage page for details on how to run GaNDLF in this \"dockerized\" form. Enable GPU usage from Docker (optional, Linux only) \u00b6 In order for \"dockerized\" GaNDLF to use your GPU, several steps are needed: Ensure sure that you have correct NVIDIA drivers for your GPU. Then, on Linux, follow the instructions to set up the NVIDIA Container Toolkit . This can be replicated for ROCm for AMD , by following the instructions to set up the ROCm Container Toolkit . On Windows \u00b6 On Windows, GPU and CUDA support requires either Windows 11, or (on Windows 10) to be registered for the Windows Insider program. If you meet those requirements and have current NVIDIA drivers , GPU support for Docker should work automatically . Otherwise, please try updating your Docker Desktop version. Note : We cannot provide support for the Windows Insider program or for Docker Desktop itself. Building your own GaNDLF Docker Image \u00b6 You may also build a Docker image of GaNDLF from the source repository. Just specify the Dockerfile for your preferred GPU-compute platform (or CPU): ( base ) $> git clone https://github.com/mlcommons/GaNDLF.git ( base ) $> cd GaNDLF ( base ) $> docker build -t gandlf: ${ mytagname } -f Dockerfile- ${ target_platform } . # change ${mytagname} and ${target_platform} as needed","title":"Installation"},{"location":"setup/#setupinstallation-instructions","text":"","title":"Setup/Installation Instructions"},{"location":"setup/#prerequisites","text":"Python3 with a preference for conda , and python version 3.9 (higher versions might work, but they are untested ). Knowledge of managing Python environments . The instructions below assume knowledge of the conda management system . Alternatively, you can run GaNDLF via Docker . This needs different prerequisites. See the Docker Installation section below for more information.","title":"Prerequisites"},{"location":"setup/#optional-requirements","text":"GPU compute (usually needed for faster training): Install appropriate drivers: NVIDIA AMD Compute toolkit appropriate for your hardware: NVIDIA: CUDA and a compatible cuDNN installed system-wide AMD: ROCm Windows: Microsoft Visual C++ 14.0 or greater . This is required for PyTorch to work on Windows. If you are using conda, you can install it using the following command for your virtual environment: conda install -c anaconda m2w64-toolchain .","title":"Optional Requirements"},{"location":"setup/#installation","text":"","title":"Installation"},{"location":"setup/#install-pytorch","text":"GaNDLF's primary computational foundation is built on PyTorch, and as such it supports all hardware types that PyTorch supports. Please install PyTorch for your hardware type before installing GaNDLF. See the PyTorch installation instructions for more details. First, instantiate your environment ( base ) $> conda create -n venv_gandlf python = 3 .11 -y ( base ) $> conda activate venv_gandlf ( venv_gandlf ) $> ### subsequent commands go here You may install pytorch to be compatible with CUDA, ROCm, or CPU-only. An exhaustive list of PyTorch installations for the specific version compatible with GaNDLF can be found here: https://pytorch.org/get-started/previous-versions/","title":"Install PyTorch"},{"location":"setup/#optional-dependencies","text":"The following dependencies are optional , and are only needed to access specific features of GaNDLF. ( venv_gandlf ) $> pip install openvino-dev == 2023 .0.1 # [OPTIONAL] to generate post-training optimized models for inference","title":"Optional Dependencies"},{"location":"setup/#install-from-package-managers","text":"This option is recommended for most users, and allows for the quickest way to get started with GaNDLF. ( venv_gandlf ) $> pip install gandlf # this will give you the latest stable release You can also use conda ( venv_gandlf ) $> conda install -c conda-forge gandlf -y If you are interested in running the latest version of GaNDLF, you can install the nightly build by running the following command: ( venv_gandlf ) $> pip install --pre gandlf You can also use conda ( venv_gandlf ) $> conda install -c conda-forge/label/gandlf_dev -c conda-forge gandlf -y","title":"Install from Package Managers"},{"location":"setup/#install-from-sources","text":"Use this option if you want to contribute to GaNDLF , or are interested to make other code-level changes for your own use. ( venv_gandlf ) $> git clone https://github.com/mlcommons/GaNDLF.git ( venv_gandlf ) $> cd GaNDLF ( venv_gandlf ) $> pip install -e . Test your installation: ( venv_gandlf ) $> gandlf verify-install","title":"Install from Sources"},{"location":"setup/#docker-installation","text":"We provide containerized versions of GaNDLF, which allows you to run GaNDLF without worrying about installation steps or dependencies.","title":"Docker Installation"},{"location":"setup/#steps-to-run-the-docker-version-of-gandlf","text":"Install the Docker Engine for your platform. GaNDLF is available from GitHub Package Registry . Several platform versions are available, including support for CUDA, ROCm, and CPU-only. Choose the one that best matches your system and drivers. For example, if you want to get the bleeding-edge GaNDLF version, and you have CUDA Toolkit v11.6, run the following command: ( base ) $> docker pull ghcr.io/mlcommons/gandlf:latest-cuda116 This will download the GaNDLF image onto your machine. See the usage page for details on how to run GaNDLF in this \"dockerized\" form.","title":"Steps to run the Docker version of GaNDLF"},{"location":"setup/#enable-gpu-usage-from-docker-optional-linux-only","text":"In order for \"dockerized\" GaNDLF to use your GPU, several steps are needed: Ensure sure that you have correct NVIDIA drivers for your GPU. Then, on Linux, follow the instructions to set up the NVIDIA Container Toolkit . This can be replicated for ROCm for AMD , by following the instructions to set up the ROCm Container Toolkit .","title":"Enable GPU usage from Docker (optional, Linux only)"},{"location":"setup/#on-windows","text":"On Windows, GPU and CUDA support requires either Windows 11, or (on Windows 10) to be registered for the Windows Insider program. If you meet those requirements and have current NVIDIA drivers , GPU support for Docker should work automatically . Otherwise, please try updating your Docker Desktop version. Note : We cannot provide support for the Windows Insider program or for Docker Desktop itself.","title":"On Windows"},{"location":"setup/#building-your-own-gandlf-docker-image","text":"You may also build a Docker image of GaNDLF from the source repository. Just specify the Dockerfile for your preferred GPU-compute platform (or CPU): ( base ) $> git clone https://github.com/mlcommons/GaNDLF.git ( base ) $> cd GaNDLF ( base ) $> docker build -t gandlf: ${ mytagname } -f Dockerfile- ${ target_platform } . # change ${mytagname} and ${target_platform} as needed","title":"Building your own GaNDLF Docker Image"},{"location":"usage/","text":"Introduction \u00b6 For any DL pipeline, the following flow needs to be performed: Data preparation Split data into training, validation, and testing Customize the training parameters A detailed data flow diagram is presented in this link . GaNDLF addresses all of these, and the information is divided as described in the following sections. Installation \u00b6 Please follow the installation instructions to install GaNDLF. When the installation is complete, you should end up with the shell that looks like the following, which indicates that the GaNDLF virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here Preparing the Data \u00b6 Anonymize Data \u00b6 A major reason why one would want to anonymize data is to ensure that trained models do not inadvertently do not encode protect health information [ 1 , 2 ]. GaNDLF can anonymize single images or a collection of images using the gandlf anonymizer command. It can be used as follows: # continue from previous shell ( venv_gandlf ) $> gandlf anonymizer # -h, --help Show help message and exit -c ./samples/config_anonymizer.yaml \\ # anonymizer configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./input_dir_or_file \\ # input directory containing series of images to anonymize or a single image -o ./output_dir_or_file # output directory to save anonymized images or a single output image file (for a DICOM to NIfTi conversion specify a .nii.gz file) Cleanup/Harmonize/Curate Data \u00b6 It is highly recommended that the dataset you want to train/infer on has been harmonized. The following requirements should be considered: Registration Within-modality co-registration [ 1 , 2 , 3 ]. OPTIONAL : Registration of all datasets to patient atlas, if applicable [ 1 , 2 , 3 ]. Intensity harmonization : Same intensity profile, i.e., normalization [ 4 , 5 , 6 , 7 ]. GaNDLF offers multiple options for intensity normalization, including Z-scoring, Min-Max scaling, and Histogram matching. Resolution harmonization : Ensures that the images have similar physical definitions (i.e., voxel/pixel size/resolution/spacing). An illustration of the impact of voxel size/resolution/spacing can be found here , and it is encourage to read this article to added context on how this issue impacts a deep learning pipeline. This functionality is available via GaNDLF's preprocessing module . Recommended tools for tackling all aforementioned curation and annotation tasks: - Cancer Imaging Phenomics Toolkit (CaPTk) - Federated Tumor Segmentation (FeTS) Front End - 3D Slicer - ITK-SNAP Offline Patch Extraction (for histology images only) \u00b6 GaNDLF can be used to convert a Whole Slide Image (WSI) with or without a corresponding label map to patches/tiles using GaNDLF\u2019s integrated patch miner, which would need the following files: A configuration file that dictates how the patches/tiles will be extracted. A sample configuration to extract patches is presented here . The options that the can be defined in the configuration are as follows: patch_size : defines the size of the patches to extract, should be a tuple type of integers (e.g., [256,256] ) or a string containing patch size in microns (e.g., [100m,100m] ). This parameter always needs to be specified. scale : scale at which operations such as tissue mask calculation happens; defaults to 16 . num_patches : defines the number of patches to extract, use -1 to mine until exhaustion; defaults to -1 . value_map : mapping RGB values in label image to integer values for training; defaults to None . read_type : either random or sequential (latter is more efficient); defaults to random . overlap_factor : Portion of patches that are allowed to overlap ( 0->1 ); defaults to 0.0 . num_workers : number of workers to use for patch extraction (note that this does not scale according to the number of threads available on your machine); defaults to 1 . A CSV file with the following columns: SubjectID : the ID of the subject for the WSI Channel_0 : the full path to the WSI file which will be used to extract patches Label : (optional) full path to the label map file Once these files are present, the patch miner can be run using the following command: # continue from previous shell ( venv_gandlf ) $> gandlf patch-miner \\ # -h, --help Show help message and exit -c ./exp_patchMiner/config.yaml \\ # patch extraction configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./exp_patchMiner/input.csv \\ # data in CSV format -o ./exp_patchMiner/output_dir/ # output directory Running preprocessing before training/inference (optional) \u00b6 Running preprocessing before training/inference is optional, but recommended. It will significantly reduce the computational footprint during training/inference at the expense of larger storage requirements. To run preprocessing before training/inference you can use the following command, which will save the processed data in ./experiment_0/output_dir/ with a new data CSV and the corresponding model configuration: # continue from previous shell ( venv_gandlf ) $> gandlf preprocess \\ # -h, --help Show help message and exit -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./experiment_0/train.csv \\ # data in CSV format -o ./experiment_0/output_dir/ # output directory Constructing the Data CSV \u00b6 This application can leverage multiple channels/modalities for training while using a multi-class segmentation file. The expected format is shown as an example in samples/sample_train.csv and needs to be structured with the following header format (which shows a CSV with N subjects, each having X channels/modalities that need to be processed): SubjectID,Channel_0,Channel_1,...,Channel_X,Label 001,/full/path/001/0.nii.gz,/full/path/001/1.nii.gz,...,/full/path/001/X.nii.gz,/full/path/001/segmentation.nii.gz 002,/full/path/002/0.nii.gz,/full/path/002/1.nii.gz,...,/full/path/002/X.nii.gz,/full/path/002/segmentation.nii.gz ... N,/full/path/N/0.nii.gz,/full/path/N/1.nii.gz,...,/full/path/N/X.nii.gz,/full/path/N/segmentation.nii.gz Notes: Channel can be substituted with Modality or Image Label can be substituted with Mask or Segmentation and is used to specify the annotation file for segmentation models For classification/regression, add a column called ValueToPredict . Currently, we are supporting only a single value prediction per model. Only a single Label or ValueToPredict header should be passed Multiple segmentation classes should be in a single file with unique label numbers. Multi-label classification/regression is currently not supported. Using the gandlf construct-csv command \u00b6 To make the process of creating the CSV easier, we have provided a gandlf construct-csv command. This script works when the data is arranged in the following format (example shown of the data directory arrangement from the Brain Tumor Segmentation (BraTS) Challenge ): $DATA_DIRECTORY \u2502 \u2514\u2500\u2500\u2500Patient_001 # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 Patient_001_brain_t1.nii.gz \u2502 \u2502 Patient_001_brain_t1ce.nii.gz \u2502 \u2502 Patient_001_brain_t2.nii.gz \u2502 \u2502 Patient_001_brain_flair.nii.gz \u2502 \u2502 Patient_001_seg.nii.gz # optional for segmentation tasks \u2502 \u2514\u2500\u2500\u2500Patient_002 # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 Patient_002_brain_t1.nii.gz \u2502 \u2502 Patient_002_brain_t1ce.nii.gz \u2502 \u2502 Patient_002_brain_t2.nii.gz \u2502 \u2502 Patient_002_brain_flair.nii.gz \u2502 \u2502 Patient_002_seg.nii.gz # optional for segmentation tasks \u2502 \u2514\u2500\u2500\u2500JaneDoe # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 randomFileName_0_t1.nii.gz # the string identifier needs to be the same for each modality \u2502 \u2502 randomFileName_1_t1ce.nii.gz \u2502 \u2502 randomFileName_2_t2.nii.gz \u2502 \u2502 randomFileName_3_flair.nii.gz \u2502 \u2502 randomFileName_seg.nii.gz # optional for segmentation tasks \u2502 ... The following command shows how the script works: # continue from previous shell ( venv_gandlf ) $> gandlf construct-csv \\ # -h, --help Show help message and exit -i $DATA_DIRECTORY # this is the main data directory -c _t1.nii.gz,_t1ce.nii.gz,_t2.nii.gz,_flair.nii.gz \\ # an example image identifier for 4 structural brain MR sequences for BraTS, and can be changed based on your data -l _seg.nii.gz \\ # an example label identifier - not needed for regression/classification, and can be changed based on your data -o ./experiment_0/train_data.csv # output CSV to be used for training Notes : For classification/regression, add a column called ValueToPredict . Currently, we are supporting only a single value prediction per model. SubjectID or PatientName is used to ensure that the randomized split is done per-subject rather than per-image. For data arrangement different to what is described above, a customized script will need to be written to generate the CSV, or you can enter the data manually into the CSV. Using the gandlf split-csv command \u00b6 To split the data CSV into training, validation, and testing CSVs, the gandlf split-csv script can be used. The following command shows how the script works: # continue from previous shell ( venv_gandlf ) $> gandlf split-csv \\ # -h, --help Show help message and exit -i ./experiment_0/train_data.csv \\ # output CSV from the `gandlf construct-csv` script -c $gandlf_config \\ # the GaNDLF config (in YAML) with the `nested_training` key specified to the folds needed -o $output_dir # the output directory to save the split data Using the --log-file parameter \u00b6 By default, only the info and error logs will be displayed in the console and the log file will be saved in $(home)/.gandlf/<timestamp>.log . Also, you can use the --log-file and provide the file that you want to save the logs ( venv_gandlf ) $> gandlf <command> --log-file <log_file_path> Customize the Training \u00b6 GaNDLF requires a YAML-based configuration that controls various aspects of the training/inference process. There are multiple samples for users to start as their baseline for further customization. A list of the available samples is presented as follows: Sample showing all the available options Segmentation example Regression example Classification example Notes : More details on the configuration options are available in the customization page . Ensure that the configuration has valid syntax by checking the file using any YAML validator such as yamlchecker.com or yamlvalidator.com before trying to train. Running multiple experiments (optional) \u00b6 The gandlf config-generator command can be used to generate a grid of configurations for tuning the hyperparameters of a baseline configuration that works for your dataset and problem. Use a strategy file (example is shown in samples/config_generator_strategy.yaml . Provide the baseline configuration which has enabled you to successfully train a model for 1 epoch for your dataset and problem at hand (regardless of the efficacy). Run the following command: # continue from previous shell ( venv_gandlf ) $> gandlf config-generator \\ # -h, --help Show help message and exit -c ./samples/config_all_options.yaml \\ # baseline configuration -s ./samples/config_generator_strategy.yaml \\ # strategy file -o ./all_experiments/ # output directory 5. For example, to generate 4 configurations that leverage unet and resunet architectures for learning rates of [0.1,0.01] , you can use the following strategy file: model : { architecture : [ unet , resunet ], } learning_rate : [ 0.1 , 0.01 ] Running GaNDLF (Training/Inference) \u00b6 You can use the following code snippet to run GaNDLF: # continue from previous shell ( venv_gandlf ) $> gandlf run \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./experiment_0/train.csv \\ # data in CSV format -m ./experiment_0/model_dir/ \\ # model directory (i.e., the `model-dir`) where the output of the training will be stored, created if not present --train \\ # --train/-t or --infer # ensure CUDA_VISIBLE_DEVICES env variable is set when using GPU # -rt , --reset # [optional] completely resets the previous run by deleting `model-dir` # -rm , --resume # [optional] resume previous training by only keeping model dict in `model-dir` Special notes for Inference for Histology images \u00b6 If you trying to perform inference on pre-extracted patches, please change the modality key in the configuration to rad . This will ensure the histology-specific pipelines are not triggered. However, if you are trying to perform inference on full WSIs, modality should be kept as histo . Generate Metrics \u00b6 GaNDLF provides a script to generate metrics after an inference process is done.The script can be used as follows: # continue from previous shell ( venv_gandlf ) $> gandlf generate-metrics \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c , --config The configuration file ( contains all the information related to the training/inference session ) -i , --input-data CSV file that is used to generate the metrics ; should contain 3 columns: 'SubjectID,Target,Prediction' -o , --output-file Location to save the output dictionary. If not provided, will print to stdout. Once you have your CSV in the specific format, you can pass it on to generate the metrics. Here is an example for segmentation: SubjectID,Target,Prediction 001,/path/to/001/target.nii.gz,/path/to/001/prediction.nii.gz 002,/path/to/002/target.nii.gz,/path/to/002/prediction.nii.gz ... Similarly, for classification or regression ( A , B , C , D are integers for classification and floats for regression): SubjectID,Target,Prediction 001,A,B 002,C,D ... Special cases \u00b6 BraTS Segmentation Metrics : To generate annotation to annotation metrics for BraTS segmentation tasks [ ref ], ensure that the config has problem_type: segmentation_brats , and the CSV can be in the same format as segmentation: SubjectID,Target,Prediction 001,/path/to/001/target_image.nii.gz,/path/to/001/prediction_image.nii.gz 002,/path/to/002/target_image.nii.gz,/path/to/002/prediction_image.nii.gz ... This can also be customized using the panoptica_config dictionary. See this sample for an example. Additionally, a more \"concise\" variant of the config is present here . BraTS Synthesis Metrics : To generate image to image metrics for synthesis tasks (including for the BraTS synthesis tasks [ 1 , 2 ]), ensure that the config has problem_type: synthesis , and the CSV can be in the same format as segmentation (note that the Mask column is optional): SubjectID,Target,Prediction,Mask 001,/path/to/001/target_image.nii.gz,/path/to/001/prediction_image.nii.gz,/path/to/001/brain_mask.nii.gz 002,/path/to/002/target_image.nii.gz,/path/to/002/prediction_image.nii.gz,/path/to/002/brain_mask.nii.gz ... GPU usage, parallelization and numerical precision \u00b6 Using GPU \u00b6 By default, GaNDLF will use GPU accelerator if CUDA_VISIBLE_DEVICES environment variable is set ref . To explicitly configure usage of GPU/CPU, you can set the accelerator key in the configuration file to cuda or cpu . By default, it is set to auto , which will use GPU if available, otherwise CPU. Multi-GPU training \u00b6 GaNDLF enables relatively straightforward multi-GPU training. Simply set the CUDA_VISIBLE_DEVICES environment variable includes the GPU IDs you want to use (see above paragraph for reference about this variable). GaNDLF will automatically distribute the training across the GPUs using Distributed Data Parallel (DDP) strategy. To explicitly set the strategy, set the strategy key in the configuration file to ddp . By default, it is set to auto , which will use DDP if multiple GPUs are available, otherwise single GPU or CPU. Multi-node training and HPC environments \u00b6 In the current release, GaNDLF does not support multi-node training. This feature will be enabled in the upcoming releases. Choosing numerical precision \u00b6 By default, GaNDLF uses float32 for training and inference computations. To change the numerical precision, set the precision key in the configuration file to one of the values [16, 32, 64, \"64-true\", \"32-true\", \"16-mixed\", \"bf16\", \"bf16-mixed\"] . Using reduced precision usually results in faster computations and lower memory usage, although in some cases it may lead to numerical instability - be sure to observe the training process and adjust the precision accordingly. Good rule of thumb is to start in default precision ( 32 ) and then experiment with lower precisions. Expected Output(s) \u00b6 Training \u00b6 Once your model is trained, you should see the following output: # continue from previous shell ( venv_gandlf ) $> ls ./experiment_0/model_dir/ data_ ${ cohort_type } .csv # data CSV used for the different cohorts, which can be either training/validation/testing data_ ${ cohort_type } .pkl # same as above, but in pickle format logs_ ${ cohort_type } .csv # logs for the different cohorts that contain the various metrics, which can be either training/validation/testing ${ architecture_name } _best.pth.tar # the best model in native PyTorch format ${ architecture_name } _latest.pth.tar # the latest model in native PyTorch format ${ architecture_name } _initial.pth.tar # the initial model in native PyTorch format ${ architecture_name } _initial. { onnx/xml/bin } # [optional] if ${architecture_name} is supported, the graph-optimized best model in ONNX format # other files dependent on if training/validation/testing output was enabled in configuration Inference \u00b6 The output of inference will be predictions based on the model that was trained. The predictions will be saved in the same directory as the model if output-dir is not passed to gandlf run . For segmentation, a directory will be created per subject ID in the input CSV. For classification/regression, the predictions will be generated in the output-dir or model-dir as a CSV file. Plot the final results \u00b6 After the testing/validation training is finished, GaNDLF enables the collection of all the statistics from the final models for testing and validation datasets and plot them. The gandlf collect-stats command can be used for plotting: # continue from previous shell ( venv_gandlf ) $> gandlf collect-stats \\ -m /path/to/trained/models \\ # directory which contains testing and validation models -o ./experiment_0/output_dir_stats/ # output directory to save stats and plot M3D-CAM usage \u00b6 The integration of the M3D-CAM library into GaNDLF enables the generation of attention maps for 3D/2D images in the validation epoch for classification and segmentation tasks. To activate M3D-CAM you just need to add the following parameter to the config: medcam : { backend : \"gcam\" , layer : \"auto\" } You can choose one of the following backends: Grad-CAM ( gcam ) Guided Backpropagation ( gbp ) Guided Grad-CAM ( ggcam ) Grad-CAM++ ( gcampp ) Optionally one can also change the name of the layer for which the attention maps should be generated. The default behavior is auto which chooses the last convolutional layer. All generated attention maps can be found in the experiment's output directory. Link to the original repository: github.com/MECLabTUDA/M3d-Cam Post-Training Model Optimization \u00b6 If you have a model previously trained using GaNDLF that you wish to run graph optimizations on, you can use the gandlf optimize-model command to do so. The following command shows how it works: # continue from previous shell ( venv_gandlf ) $> gandlf optimize-model \\ -m /path/to/trained/ ${ architecture_name } _best.pth.tar # directory which contains testing and validation models If ${architecture_name} is supported, the optimized model will get generated in the model directory, with the name ${architecture_name}_optimized.onnx . Deployment \u00b6 Deploy as a Model (WARNING - this feature is not fully supported in the current release!) \u00b6 GaNDLF provides the ability to deploy models into easy-to-share, easy-to-use formats -- users of your model do not even need to install GaNDLF. Currently, Docker images are supported (which can be converted to Apptainer/Singularity format ). These images meet the MLCube interface . This allows your algorithm to be used in a consistent manner with other machine learning tools. The resulting image contains your specific version of GaNDLF (including any custom changes you have made) and your trained model and configuration. This ensures that upstream changes to GaNDLF will not break compatibility with your model. To deploy a model, simply run the gandlf deploy command after training a model. You will need the Docker engine installed to build Docker images. This will create the image and, for MLCubes, generate an MLCube directory complete with an mlcube.yaml specifications file, along with the workspace directory copied from a pre-existing template. # continue from previous shell ( venv_gandlf ) $> gandlf deploy \\ # -h, --help Show help message and exit -c ./experiment_0/model.yaml \\ # Configuration to bundle with the model (you can recover it with `gandlf recover-config` first if needed) -m ./experiment_0/model_dir/ \\ # model directory (i.e., modeldir) --target docker \\ # the target platform (--help will show all available targets) --mlcube-root ./my_new_mlcube_dir \\ # Directory containing mlcube.yaml (used to configure your image base) -o ./output_dir # Output directory where a new mlcube.yaml file to be distributed with your image will be created --mlcube-type model # deploy as a model MLCube. Deploy as a Metrics Generator \u00b6 You can also deploy GaNDLF as a metrics generator (see the Generate Metrics section) as follows: ( venv_gandlf ) $> gandlf deploy \\ ## -h, --help show help message and exit --target docker \\ # the target platform (--help will show all available targets) --mlcube-root ./my_new_mlcube_dir \\ # Directory containing mlcube.yaml (used to configure your image base) -o ./output_dir # Output directory where a new mlcube.yaml file to be distributed with your image will be created -e ./my_custom_script.py # An optional custom script used as an entrypoint for your MLCube --mlcube-type metrics # deploy as a metrics MLCube. The resulting MLCube can be used to calculate any metrics supported in GaNDLF. You can configure which metrics to be calculated by passing a GaNDLF config file when running the MLCube. For more information about using a custom entrypoint script, see the examples here . Federating your model using OpenFL \u00b6 Once you have a model definition, it is easy to perform federated learning using the Open Federated Learning (OpenFL) library . Follow the tutorial in this page to get started. Federating your model evaluation using MedPerf \u00b6 Once you have a trained model, you can perform federated evaluation using MedPerf . Follow the tutorial in this page to get started. Notes : - To create a GaNDLF MLCube, for technical reasons, you need write access to the GaNDLF package. This should be automatic while using a virtual environment that you have set up. See the installation instructions for details. - This needs GaNDLF to be initialized as an MLCube. See the mlcube instructions for details. Running with Docker \u00b6 The usage of GaNDLF remains generally the same even from Docker, but there are a few extra considerations. Once you have pulled the GaNDLF image, it will have a tag, such as cbica/gandlf:latest-cpu . Run the following command to list your images and ensure GaNDLF is present: ( main ) $> docker image ls You can invoke docker run with the appropriate tag to run GaNDLF: ( main ) $> docker run -it --rm --name gandlf cbica/gandlf:latest-cpu ${ gandlf command and parameters go here! } Remember that arguments/options for Docker itself go before the image tag, while the command and arguments for GaNDLF go after the image tag. For more details and options, see the Docker run documentation . However, most commands that require files or directories as input or output will fail, because the container, by default, cannot read or write files on your machine for security considerations . In order to fix this, you need to mount specific locations in the filesystem . Mounting Input and Output \u00b6 The container is basically a filesystem of its own. To make your data available to the container, you will need to mount in files and directories. Generally, it is useful to mount at least input directory (as read-only) and an output directory. See the Docker bind mount instructions for more information. For example, you might run: ( main ) $> docker run -it --rm --name gandlf --volume /home/researcher/gandlf_input:/input:ro --volume /home/researcher/gandlf_output:/output cbica/gandlf:latest-cpu [ command and args go here ] Remember that the process running in the container only considers the filesystem inside the container, which is structured differently from that of your host machine. Therefore, you will need to give paths relative to the mount point destination . Additionally, any paths used internally by GaNDLF will refer to locations inside the container. This means that data CSVs produced by the gandlf construct-csv command will need to be made from the container and with input in the same locations. Expanding on our last example: ( main ) $> docker run -it --rm --name dataprep \\ --volume /home/researcher/gandlf_input:/input:ro \\ # input data is mounted as read-only --volume /home/researcher/gandlf_output:/output \\ # output data is mounted as read-write cbica/gandlf:latest-cpu \\ # change to appropriate docker image tag construct-csv \\ # standard construct CSV API starts --input-dir /input/data \\ --output-file /output/data.csv \\ --channels-id _t1.nii.gz \\ --label-id _seg.nii.gz The previous command will generate a data CSV file that you can safely edit outside the container (such as by adding a ValueToPredict column). Then, you can refer to the same file when running again: ( main ) $> docker run -it --rm --name training \\ --volume /home/researcher/gandlf_input:/input:ro \\ # input data is mounted as read-only --volume /home/researcher/gandlf_output:/output \\ # output data is mounted as read-write cbica/gandlf:latest-cpu \\ # change to appropriate docker image tag gandlf run --train \\ # standard training API starts --config /input/config.yml \\ --inputdata /output/data.csv \\ --modeldir /output/model Special Case for Training \u00b6 Considering that you want to train on an existing model that is inside the GaNDLF container (such as in an MLCube container created by gandlf deploy ), the output will be to a location embedded inside the container. Since you cannot mount something into that spot without overwriting the model, you can instead use the built-in docker cp command to extract the model afterward. For example, you can fine-tune a model on your own data using the following commands as a starting point: # Run training on your new data ( main ) $> docker run --name gandlf_training mlcommons/gandlf-pretrained:0.0.1 -v /my/input/data:/input gandlf run -m /embedded_model/ [ ... ] # Do not include \"--rm\" option! # Copy the finetuned model out of the container, to a location on the host ( main ) $> docker cp gandlf_training:/embedded_model /home/researcher/extracted_model # Now you can remove the container to clean up ( main ) $> docker rm -f gandlf_training Enabling GPUs \u00b6 Some special arguments need to be passed to Docker to enable it to use your GPU. With Docker version > 19.03 You can use docker run --gpus all to expose all GPUs to the container. See the NVIDIA Docker documentation for more details. If using CUDA, GaNDLF also expects the environment variable CUDA_VISIBLE_DEVICES to be set. To use the same settings as your host machine, simply add -e CUDA_VISIBLE_DEVICES to your docker run command. For example: For example: ( main ) $> docker run --gpus all -e CUDA_VISIBLE_DEVICES -it --rm --name gandlf cbica/gandlf:latest-cuda113 gandlf run --device cuda [ ... ] This can be replicated for ROCm for AMD , by following the instructions to set up the ROCm Container Toolkit . MLCubes \u00b6 GaNDLF, and GaNDLF-created models, may be distributed as an MLCube . This involves distributing an mlcube.yaml file. That file can be specified when using the MLCube runners . The runner will perform many aspects of configuring your container for you. Currently, only the mlcube_docker runner is supported. See the MLCube documentation for more details. HuggingFace CLI \u00b6 This tool allows you to interact with the Hugging Face Hub directly from a terminal. For example, you can create a repository, upload and download files, etc. Download an entire repository \u00b6 GaNDLF's Hugging Face CLI allows you to download repositories through the command line. This can be done by just specifying the repo id: ( main ) $> gandlf hf --download --repo-id HuggingFaceH4/zephyr-7b-beta Apart from the Repo Id you can also provide other arguments. Revision \u00b6 To download from a specific revision (commit hash, branch name or tag), use the --revision option: ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1 Specify a token \u00b6 To access private or gated repositories, you must use a token. You can do this using the --token option: ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1 --token hf_**** Specify cache directory \u00b6 If not using --local-dir, all files will be downloaded by default to the cache directory defined by the HF_HOME environment variable. You can specify a custom cache using --cache-dir: ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1 --token hf_**** --cache-dir ./path/to/cache Download to a local folder \u00b6 The recommended (and default) way to download files from the Hub is to use the cache-system. However, in some cases you want to download files and move them to a specific folder. This is useful to get a workflow closer to what git commands offer. You can do that using the --local-dir option. A ./huggingface/ folder is created at the root of your local directory containing metadata about the downloaded files. This prevents re-downloading files if they\u2019re already up-to-date. If the metadata has changed, then the new file version is downloaded. This makes the local-dir optimized for pulling only the latest changes. ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1 --token hf_**** --cache-dir ./path/to/cache --local-dir ./path/to/dir Force Download \u00b6 To specify if the files should be downloaded even if it already exists in the local cache. ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1 --token hf_**** --cache-dir ./path/to/cache --local-dir ./path/to/dir --force-download Upload an entire folder \u00b6 Use the gandlf hf --upload upload command to upload files to the Hub directly. ( main ) $> gandlf hf --upload --repo-id Wauplin/my-cool-model --folder-path ./model --token hf_**** Upload to a Specific Path in Repo \u00b6 Relative path of the directory in the repo. Will default to the root folder of the repository. ( main ) $> gandlf hf --upload --repo-id Wauplin/my-cool-model --folder-path ./model/data --path-in-repo ./data --token hf_**** Upload multiple files \u00b6 To upload multiple files from a folder at once without uploading the entire folder, use the --allow-patterns and --ignore-patterns patterns. It can also be combined with the --delete-patterns option to delete files on the repo while uploading new ones. In the example below, we sync the local Space by deleting remote files and uploading all files except the ones in /logs: ( main ) $> gandlf hf Wauplin/space-example --repo-type = space --exclude = \"/logs/*\" --delete = \"*\" --commit-message = \"Sync local Space with Hub\" Specify a token \u00b6 To upload files, you must use a token. By default, the token saved locally will be used. If you want to authenticate explicitly, use the --token option: ( main ) $>gandlf hf --upload Wauplin/my-cool-model --folder-path ./model --token = hf_**** Specify a commit message \u00b6 Use the --commit-message and --commit-description to set a custom message and description for your commit instead of the default one ( main ) $>gandlf hf --upload Wauplin/my-cool-model --folder-path ./model --token = hf_**** --commit-message \"Epoch 34/50\" --commit-description = \"Val accuracy: 68%. Check tensorboard for more details.\" Upload to a dataset or Space \u00b6 To upload to a dataset or a Space, use the --repo-type option: ( main ) $>gandlf hf --upload Wauplin/my-cool-model --folder-path ./model --token = hf_**** --repo-type dataset Huggingface Template For Upload \u00b6 Design and Modify Template \u00b6 To design the huggingface template use the hugging_face.md file change the mandatory field [REQUIRED_FOR_GANDLF] to it's respective name don't leave it blank other wise it may through error, other field can be modeified by the user as per his convenience # Here the required field change from [REQUIRED_FOR_GANDLF] to [GANDLF] **Developed by:** {{ developers | default ( \"[GANDLF]\" , true )}} Mentioned The Huggingface Template \u00b6 To mentioned the Huggingface Template , use the --hf-template option: ( main ) $>gandlf hf --upload Wauplin/my-cool-model --folder-path ./model --token = hf_**** --repo-type dataset --hf-template /hugging_face.md","title":"Usage"},{"location":"usage/#introduction","text":"For any DL pipeline, the following flow needs to be performed: Data preparation Split data into training, validation, and testing Customize the training parameters A detailed data flow diagram is presented in this link . GaNDLF addresses all of these, and the information is divided as described in the following sections.","title":"Introduction"},{"location":"usage/#installation","text":"Please follow the installation instructions to install GaNDLF. When the installation is complete, you should end up with the shell that looks like the following, which indicates that the GaNDLF virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here","title":"Installation"},{"location":"usage/#preparing-the-data","text":"","title":"Preparing the Data"},{"location":"usage/#anonymize-data","text":"A major reason why one would want to anonymize data is to ensure that trained models do not inadvertently do not encode protect health information [ 1 , 2 ]. GaNDLF can anonymize single images or a collection of images using the gandlf anonymizer command. It can be used as follows: # continue from previous shell ( venv_gandlf ) $> gandlf anonymizer # -h, --help Show help message and exit -c ./samples/config_anonymizer.yaml \\ # anonymizer configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./input_dir_or_file \\ # input directory containing series of images to anonymize or a single image -o ./output_dir_or_file # output directory to save anonymized images or a single output image file (for a DICOM to NIfTi conversion specify a .nii.gz file)","title":"Anonymize Data"},{"location":"usage/#cleanupharmonizecurate-data","text":"It is highly recommended that the dataset you want to train/infer on has been harmonized. The following requirements should be considered: Registration Within-modality co-registration [ 1 , 2 , 3 ]. OPTIONAL : Registration of all datasets to patient atlas, if applicable [ 1 , 2 , 3 ]. Intensity harmonization : Same intensity profile, i.e., normalization [ 4 , 5 , 6 , 7 ]. GaNDLF offers multiple options for intensity normalization, including Z-scoring, Min-Max scaling, and Histogram matching. Resolution harmonization : Ensures that the images have similar physical definitions (i.e., voxel/pixel size/resolution/spacing). An illustration of the impact of voxel size/resolution/spacing can be found here , and it is encourage to read this article to added context on how this issue impacts a deep learning pipeline. This functionality is available via GaNDLF's preprocessing module . Recommended tools for tackling all aforementioned curation and annotation tasks: - Cancer Imaging Phenomics Toolkit (CaPTk) - Federated Tumor Segmentation (FeTS) Front End - 3D Slicer - ITK-SNAP","title":"Cleanup/Harmonize/Curate Data"},{"location":"usage/#offline-patch-extraction-for-histology-images-only","text":"GaNDLF can be used to convert a Whole Slide Image (WSI) with or without a corresponding label map to patches/tiles using GaNDLF\u2019s integrated patch miner, which would need the following files: A configuration file that dictates how the patches/tiles will be extracted. A sample configuration to extract patches is presented here . The options that the can be defined in the configuration are as follows: patch_size : defines the size of the patches to extract, should be a tuple type of integers (e.g., [256,256] ) or a string containing patch size in microns (e.g., [100m,100m] ). This parameter always needs to be specified. scale : scale at which operations such as tissue mask calculation happens; defaults to 16 . num_patches : defines the number of patches to extract, use -1 to mine until exhaustion; defaults to -1 . value_map : mapping RGB values in label image to integer values for training; defaults to None . read_type : either random or sequential (latter is more efficient); defaults to random . overlap_factor : Portion of patches that are allowed to overlap ( 0->1 ); defaults to 0.0 . num_workers : number of workers to use for patch extraction (note that this does not scale according to the number of threads available on your machine); defaults to 1 . A CSV file with the following columns: SubjectID : the ID of the subject for the WSI Channel_0 : the full path to the WSI file which will be used to extract patches Label : (optional) full path to the label map file Once these files are present, the patch miner can be run using the following command: # continue from previous shell ( venv_gandlf ) $> gandlf patch-miner \\ # -h, --help Show help message and exit -c ./exp_patchMiner/config.yaml \\ # patch extraction configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./exp_patchMiner/input.csv \\ # data in CSV format -o ./exp_patchMiner/output_dir/ # output directory","title":"Offline Patch Extraction (for histology images only)"},{"location":"usage/#running-preprocessing-before-traininginference-optional","text":"Running preprocessing before training/inference is optional, but recommended. It will significantly reduce the computational footprint during training/inference at the expense of larger storage requirements. To run preprocessing before training/inference you can use the following command, which will save the processed data in ./experiment_0/output_dir/ with a new data CSV and the corresponding model configuration: # continue from previous shell ( venv_gandlf ) $> gandlf preprocess \\ # -h, --help Show help message and exit -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./experiment_0/train.csv \\ # data in CSV format -o ./experiment_0/output_dir/ # output directory","title":"Running preprocessing before training/inference (optional)"},{"location":"usage/#constructing-the-data-csv","text":"This application can leverage multiple channels/modalities for training while using a multi-class segmentation file. The expected format is shown as an example in samples/sample_train.csv and needs to be structured with the following header format (which shows a CSV with N subjects, each having X channels/modalities that need to be processed): SubjectID,Channel_0,Channel_1,...,Channel_X,Label 001,/full/path/001/0.nii.gz,/full/path/001/1.nii.gz,...,/full/path/001/X.nii.gz,/full/path/001/segmentation.nii.gz 002,/full/path/002/0.nii.gz,/full/path/002/1.nii.gz,...,/full/path/002/X.nii.gz,/full/path/002/segmentation.nii.gz ... N,/full/path/N/0.nii.gz,/full/path/N/1.nii.gz,...,/full/path/N/X.nii.gz,/full/path/N/segmentation.nii.gz Notes: Channel can be substituted with Modality or Image Label can be substituted with Mask or Segmentation and is used to specify the annotation file for segmentation models For classification/regression, add a column called ValueToPredict . Currently, we are supporting only a single value prediction per model. Only a single Label or ValueToPredict header should be passed Multiple segmentation classes should be in a single file with unique label numbers. Multi-label classification/regression is currently not supported.","title":"Constructing the Data CSV"},{"location":"usage/#using-the-gandlf-construct-csv-command","text":"To make the process of creating the CSV easier, we have provided a gandlf construct-csv command. This script works when the data is arranged in the following format (example shown of the data directory arrangement from the Brain Tumor Segmentation (BraTS) Challenge ): $DATA_DIRECTORY \u2502 \u2514\u2500\u2500\u2500Patient_001 # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 Patient_001_brain_t1.nii.gz \u2502 \u2502 Patient_001_brain_t1ce.nii.gz \u2502 \u2502 Patient_001_brain_t2.nii.gz \u2502 \u2502 Patient_001_brain_flair.nii.gz \u2502 \u2502 Patient_001_seg.nii.gz # optional for segmentation tasks \u2502 \u2514\u2500\u2500\u2500Patient_002 # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 Patient_002_brain_t1.nii.gz \u2502 \u2502 Patient_002_brain_t1ce.nii.gz \u2502 \u2502 Patient_002_brain_t2.nii.gz \u2502 \u2502 Patient_002_brain_flair.nii.gz \u2502 \u2502 Patient_002_seg.nii.gz # optional for segmentation tasks \u2502 \u2514\u2500\u2500\u2500JaneDoe # this is constructed from the ${PatientID} header of CSV \u2502 \u2502 randomFileName_0_t1.nii.gz # the string identifier needs to be the same for each modality \u2502 \u2502 randomFileName_1_t1ce.nii.gz \u2502 \u2502 randomFileName_2_t2.nii.gz \u2502 \u2502 randomFileName_3_flair.nii.gz \u2502 \u2502 randomFileName_seg.nii.gz # optional for segmentation tasks \u2502 ... The following command shows how the script works: # continue from previous shell ( venv_gandlf ) $> gandlf construct-csv \\ # -h, --help Show help message and exit -i $DATA_DIRECTORY # this is the main data directory -c _t1.nii.gz,_t1ce.nii.gz,_t2.nii.gz,_flair.nii.gz \\ # an example image identifier for 4 structural brain MR sequences for BraTS, and can be changed based on your data -l _seg.nii.gz \\ # an example label identifier - not needed for regression/classification, and can be changed based on your data -o ./experiment_0/train_data.csv # output CSV to be used for training Notes : For classification/regression, add a column called ValueToPredict . Currently, we are supporting only a single value prediction per model. SubjectID or PatientName is used to ensure that the randomized split is done per-subject rather than per-image. For data arrangement different to what is described above, a customized script will need to be written to generate the CSV, or you can enter the data manually into the CSV.","title":"Using the gandlf construct-csv command"},{"location":"usage/#using-the-gandlf-split-csv-command","text":"To split the data CSV into training, validation, and testing CSVs, the gandlf split-csv script can be used. The following command shows how the script works: # continue from previous shell ( venv_gandlf ) $> gandlf split-csv \\ # -h, --help Show help message and exit -i ./experiment_0/train_data.csv \\ # output CSV from the `gandlf construct-csv` script -c $gandlf_config \\ # the GaNDLF config (in YAML) with the `nested_training` key specified to the folds needed -o $output_dir # the output directory to save the split data","title":"Using the gandlf split-csv command"},{"location":"usage/#using-the-log-file-parameter","text":"By default, only the info and error logs will be displayed in the console and the log file will be saved in $(home)/.gandlf/<timestamp>.log . Also, you can use the --log-file and provide the file that you want to save the logs ( venv_gandlf ) $> gandlf <command> --log-file <log_file_path>","title":"Using the --log-file parameter"},{"location":"usage/#customize-the-training","text":"GaNDLF requires a YAML-based configuration that controls various aspects of the training/inference process. There are multiple samples for users to start as their baseline for further customization. A list of the available samples is presented as follows: Sample showing all the available options Segmentation example Regression example Classification example Notes : More details on the configuration options are available in the customization page . Ensure that the configuration has valid syntax by checking the file using any YAML validator such as yamlchecker.com or yamlvalidator.com before trying to train.","title":"Customize the Training"},{"location":"usage/#running-multiple-experiments-optional","text":"The gandlf config-generator command can be used to generate a grid of configurations for tuning the hyperparameters of a baseline configuration that works for your dataset and problem. Use a strategy file (example is shown in samples/config_generator_strategy.yaml . Provide the baseline configuration which has enabled you to successfully train a model for 1 epoch for your dataset and problem at hand (regardless of the efficacy). Run the following command: # continue from previous shell ( venv_gandlf ) $> gandlf config-generator \\ # -h, --help Show help message and exit -c ./samples/config_all_options.yaml \\ # baseline configuration -s ./samples/config_generator_strategy.yaml \\ # strategy file -o ./all_experiments/ # output directory 5. For example, to generate 4 configurations that leverage unet and resunet architectures for learning rates of [0.1,0.01] , you can use the following strategy file: model : { architecture : [ unet , resunet ], } learning_rate : [ 0.1 , 0.01 ]","title":"Running multiple experiments (optional)"},{"location":"usage/#running-gandlf-traininginference","text":"You can use the following code snippet to run GaNDLF: # continue from previous shell ( venv_gandlf ) $> gandlf run \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -i ./experiment_0/train.csv \\ # data in CSV format -m ./experiment_0/model_dir/ \\ # model directory (i.e., the `model-dir`) where the output of the training will be stored, created if not present --train \\ # --train/-t or --infer # ensure CUDA_VISIBLE_DEVICES env variable is set when using GPU # -rt , --reset # [optional] completely resets the previous run by deleting `model-dir` # -rm , --resume # [optional] resume previous training by only keeping model dict in `model-dir`","title":"Running GaNDLF (Training/Inference)"},{"location":"usage/#special-notes-for-inference-for-histology-images","text":"If you trying to perform inference on pre-extracted patches, please change the modality key in the configuration to rad . This will ensure the histology-specific pipelines are not triggered. However, if you are trying to perform inference on full WSIs, modality should be kept as histo .","title":"Special notes for Inference for Histology images"},{"location":"usage/#generate-metrics","text":"GaNDLF provides a script to generate metrics after an inference process is done.The script can be used as follows: # continue from previous shell ( venv_gandlf ) $> gandlf generate-metrics \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c , --config The configuration file ( contains all the information related to the training/inference session ) -i , --input-data CSV file that is used to generate the metrics ; should contain 3 columns: 'SubjectID,Target,Prediction' -o , --output-file Location to save the output dictionary. If not provided, will print to stdout. Once you have your CSV in the specific format, you can pass it on to generate the metrics. Here is an example for segmentation: SubjectID,Target,Prediction 001,/path/to/001/target.nii.gz,/path/to/001/prediction.nii.gz 002,/path/to/002/target.nii.gz,/path/to/002/prediction.nii.gz ... Similarly, for classification or regression ( A , B , C , D are integers for classification and floats for regression): SubjectID,Target,Prediction 001,A,B 002,C,D ...","title":"Generate Metrics"},{"location":"usage/#special-cases","text":"BraTS Segmentation Metrics : To generate annotation to annotation metrics for BraTS segmentation tasks [ ref ], ensure that the config has problem_type: segmentation_brats , and the CSV can be in the same format as segmentation: SubjectID,Target,Prediction 001,/path/to/001/target_image.nii.gz,/path/to/001/prediction_image.nii.gz 002,/path/to/002/target_image.nii.gz,/path/to/002/prediction_image.nii.gz ... This can also be customized using the panoptica_config dictionary. See this sample for an example. Additionally, a more \"concise\" variant of the config is present here . BraTS Synthesis Metrics : To generate image to image metrics for synthesis tasks (including for the BraTS synthesis tasks [ 1 , 2 ]), ensure that the config has problem_type: synthesis , and the CSV can be in the same format as segmentation (note that the Mask column is optional): SubjectID,Target,Prediction,Mask 001,/path/to/001/target_image.nii.gz,/path/to/001/prediction_image.nii.gz,/path/to/001/brain_mask.nii.gz 002,/path/to/002/target_image.nii.gz,/path/to/002/prediction_image.nii.gz,/path/to/002/brain_mask.nii.gz ...","title":"Special cases"},{"location":"usage/#gpu-usage-parallelization-and-numerical-precision","text":"","title":"GPU usage, parallelization and numerical precision"},{"location":"usage/#using-gpu","text":"By default, GaNDLF will use GPU accelerator if CUDA_VISIBLE_DEVICES environment variable is set ref . To explicitly configure usage of GPU/CPU, you can set the accelerator key in the configuration file to cuda or cpu . By default, it is set to auto , which will use GPU if available, otherwise CPU.","title":"Using GPU"},{"location":"usage/#multi-gpu-training","text":"GaNDLF enables relatively straightforward multi-GPU training. Simply set the CUDA_VISIBLE_DEVICES environment variable includes the GPU IDs you want to use (see above paragraph for reference about this variable). GaNDLF will automatically distribute the training across the GPUs using Distributed Data Parallel (DDP) strategy. To explicitly set the strategy, set the strategy key in the configuration file to ddp . By default, it is set to auto , which will use DDP if multiple GPUs are available, otherwise single GPU or CPU.","title":"Multi-GPU training"},{"location":"usage/#multi-node-training-and-hpc-environments","text":"In the current release, GaNDLF does not support multi-node training. This feature will be enabled in the upcoming releases.","title":"Multi-node training and HPC environments"},{"location":"usage/#choosing-numerical-precision","text":"By default, GaNDLF uses float32 for training and inference computations. To change the numerical precision, set the precision key in the configuration file to one of the values [16, 32, 64, \"64-true\", \"32-true\", \"16-mixed\", \"bf16\", \"bf16-mixed\"] . Using reduced precision usually results in faster computations and lower memory usage, although in some cases it may lead to numerical instability - be sure to observe the training process and adjust the precision accordingly. Good rule of thumb is to start in default precision ( 32 ) and then experiment with lower precisions.","title":"Choosing numerical precision"},{"location":"usage/#expected-outputs","text":"","title":"Expected Output(s)"},{"location":"usage/#training","text":"Once your model is trained, you should see the following output: # continue from previous shell ( venv_gandlf ) $> ls ./experiment_0/model_dir/ data_ ${ cohort_type } .csv # data CSV used for the different cohorts, which can be either training/validation/testing data_ ${ cohort_type } .pkl # same as above, but in pickle format logs_ ${ cohort_type } .csv # logs for the different cohorts that contain the various metrics, which can be either training/validation/testing ${ architecture_name } _best.pth.tar # the best model in native PyTorch format ${ architecture_name } _latest.pth.tar # the latest model in native PyTorch format ${ architecture_name } _initial.pth.tar # the initial model in native PyTorch format ${ architecture_name } _initial. { onnx/xml/bin } # [optional] if ${architecture_name} is supported, the graph-optimized best model in ONNX format # other files dependent on if training/validation/testing output was enabled in configuration","title":"Training"},{"location":"usage/#inference","text":"The output of inference will be predictions based on the model that was trained. The predictions will be saved in the same directory as the model if output-dir is not passed to gandlf run . For segmentation, a directory will be created per subject ID in the input CSV. For classification/regression, the predictions will be generated in the output-dir or model-dir as a CSV file.","title":"Inference"},{"location":"usage/#plot-the-final-results","text":"After the testing/validation training is finished, GaNDLF enables the collection of all the statistics from the final models for testing and validation datasets and plot them. The gandlf collect-stats command can be used for plotting: # continue from previous shell ( venv_gandlf ) $> gandlf collect-stats \\ -m /path/to/trained/models \\ # directory which contains testing and validation models -o ./experiment_0/output_dir_stats/ # output directory to save stats and plot","title":"Plot the final results"},{"location":"usage/#m3d-cam-usage","text":"The integration of the M3D-CAM library into GaNDLF enables the generation of attention maps for 3D/2D images in the validation epoch for classification and segmentation tasks. To activate M3D-CAM you just need to add the following parameter to the config: medcam : { backend : \"gcam\" , layer : \"auto\" } You can choose one of the following backends: Grad-CAM ( gcam ) Guided Backpropagation ( gbp ) Guided Grad-CAM ( ggcam ) Grad-CAM++ ( gcampp ) Optionally one can also change the name of the layer for which the attention maps should be generated. The default behavior is auto which chooses the last convolutional layer. All generated attention maps can be found in the experiment's output directory. Link to the original repository: github.com/MECLabTUDA/M3d-Cam","title":"M3D-CAM usage"},{"location":"usage/#post-training-model-optimization","text":"If you have a model previously trained using GaNDLF that you wish to run graph optimizations on, you can use the gandlf optimize-model command to do so. The following command shows how it works: # continue from previous shell ( venv_gandlf ) $> gandlf optimize-model \\ -m /path/to/trained/ ${ architecture_name } _best.pth.tar # directory which contains testing and validation models If ${architecture_name} is supported, the optimized model will get generated in the model directory, with the name ${architecture_name}_optimized.onnx .","title":"Post-Training Model Optimization"},{"location":"usage/#deployment","text":"","title":"Deployment"},{"location":"usage/#deploy-as-a-model-warning-this-feature-is-not-fully-supported-in-the-current-release","text":"GaNDLF provides the ability to deploy models into easy-to-share, easy-to-use formats -- users of your model do not even need to install GaNDLF. Currently, Docker images are supported (which can be converted to Apptainer/Singularity format ). These images meet the MLCube interface . This allows your algorithm to be used in a consistent manner with other machine learning tools. The resulting image contains your specific version of GaNDLF (including any custom changes you have made) and your trained model and configuration. This ensures that upstream changes to GaNDLF will not break compatibility with your model. To deploy a model, simply run the gandlf deploy command after training a model. You will need the Docker engine installed to build Docker images. This will create the image and, for MLCubes, generate an MLCube directory complete with an mlcube.yaml specifications file, along with the workspace directory copied from a pre-existing template. # continue from previous shell ( venv_gandlf ) $> gandlf deploy \\ # -h, --help Show help message and exit -c ./experiment_0/model.yaml \\ # Configuration to bundle with the model (you can recover it with `gandlf recover-config` first if needed) -m ./experiment_0/model_dir/ \\ # model directory (i.e., modeldir) --target docker \\ # the target platform (--help will show all available targets) --mlcube-root ./my_new_mlcube_dir \\ # Directory containing mlcube.yaml (used to configure your image base) -o ./output_dir # Output directory where a new mlcube.yaml file to be distributed with your image will be created --mlcube-type model # deploy as a model MLCube.","title":"Deploy as a Model (WARNING - this feature is not fully supported in the current release!)"},{"location":"usage/#deploy-as-a-metrics-generator","text":"You can also deploy GaNDLF as a metrics generator (see the Generate Metrics section) as follows: ( venv_gandlf ) $> gandlf deploy \\ ## -h, --help show help message and exit --target docker \\ # the target platform (--help will show all available targets) --mlcube-root ./my_new_mlcube_dir \\ # Directory containing mlcube.yaml (used to configure your image base) -o ./output_dir # Output directory where a new mlcube.yaml file to be distributed with your image will be created -e ./my_custom_script.py # An optional custom script used as an entrypoint for your MLCube --mlcube-type metrics # deploy as a metrics MLCube. The resulting MLCube can be used to calculate any metrics supported in GaNDLF. You can configure which metrics to be calculated by passing a GaNDLF config file when running the MLCube. For more information about using a custom entrypoint script, see the examples here .","title":"Deploy as a Metrics Generator"},{"location":"usage/#federating-your-model-using-openfl","text":"Once you have a model definition, it is easy to perform federated learning using the Open Federated Learning (OpenFL) library . Follow the tutorial in this page to get started.","title":"Federating your model using OpenFL"},{"location":"usage/#federating-your-model-evaluation-using-medperf","text":"Once you have a trained model, you can perform federated evaluation using MedPerf . Follow the tutorial in this page to get started. Notes : - To create a GaNDLF MLCube, for technical reasons, you need write access to the GaNDLF package. This should be automatic while using a virtual environment that you have set up. See the installation instructions for details. - This needs GaNDLF to be initialized as an MLCube. See the mlcube instructions for details.","title":"Federating your model evaluation using MedPerf"},{"location":"usage/#running-with-docker","text":"The usage of GaNDLF remains generally the same even from Docker, but there are a few extra considerations. Once you have pulled the GaNDLF image, it will have a tag, such as cbica/gandlf:latest-cpu . Run the following command to list your images and ensure GaNDLF is present: ( main ) $> docker image ls You can invoke docker run with the appropriate tag to run GaNDLF: ( main ) $> docker run -it --rm --name gandlf cbica/gandlf:latest-cpu ${ gandlf command and parameters go here! } Remember that arguments/options for Docker itself go before the image tag, while the command and arguments for GaNDLF go after the image tag. For more details and options, see the Docker run documentation . However, most commands that require files or directories as input or output will fail, because the container, by default, cannot read or write files on your machine for security considerations . In order to fix this, you need to mount specific locations in the filesystem .","title":"Running with Docker"},{"location":"usage/#mounting-input-and-output","text":"The container is basically a filesystem of its own. To make your data available to the container, you will need to mount in files and directories. Generally, it is useful to mount at least input directory (as read-only) and an output directory. See the Docker bind mount instructions for more information. For example, you might run: ( main ) $> docker run -it --rm --name gandlf --volume /home/researcher/gandlf_input:/input:ro --volume /home/researcher/gandlf_output:/output cbica/gandlf:latest-cpu [ command and args go here ] Remember that the process running in the container only considers the filesystem inside the container, which is structured differently from that of your host machine. Therefore, you will need to give paths relative to the mount point destination . Additionally, any paths used internally by GaNDLF will refer to locations inside the container. This means that data CSVs produced by the gandlf construct-csv command will need to be made from the container and with input in the same locations. Expanding on our last example: ( main ) $> docker run -it --rm --name dataprep \\ --volume /home/researcher/gandlf_input:/input:ro \\ # input data is mounted as read-only --volume /home/researcher/gandlf_output:/output \\ # output data is mounted as read-write cbica/gandlf:latest-cpu \\ # change to appropriate docker image tag construct-csv \\ # standard construct CSV API starts --input-dir /input/data \\ --output-file /output/data.csv \\ --channels-id _t1.nii.gz \\ --label-id _seg.nii.gz The previous command will generate a data CSV file that you can safely edit outside the container (such as by adding a ValueToPredict column). Then, you can refer to the same file when running again: ( main ) $> docker run -it --rm --name training \\ --volume /home/researcher/gandlf_input:/input:ro \\ # input data is mounted as read-only --volume /home/researcher/gandlf_output:/output \\ # output data is mounted as read-write cbica/gandlf:latest-cpu \\ # change to appropriate docker image tag gandlf run --train \\ # standard training API starts --config /input/config.yml \\ --inputdata /output/data.csv \\ --modeldir /output/model","title":"Mounting Input and Output"},{"location":"usage/#special-case-for-training","text":"Considering that you want to train on an existing model that is inside the GaNDLF container (such as in an MLCube container created by gandlf deploy ), the output will be to a location embedded inside the container. Since you cannot mount something into that spot without overwriting the model, you can instead use the built-in docker cp command to extract the model afterward. For example, you can fine-tune a model on your own data using the following commands as a starting point: # Run training on your new data ( main ) $> docker run --name gandlf_training mlcommons/gandlf-pretrained:0.0.1 -v /my/input/data:/input gandlf run -m /embedded_model/ [ ... ] # Do not include \"--rm\" option! # Copy the finetuned model out of the container, to a location on the host ( main ) $> docker cp gandlf_training:/embedded_model /home/researcher/extracted_model # Now you can remove the container to clean up ( main ) $> docker rm -f gandlf_training","title":"Special Case for Training"},{"location":"usage/#enabling-gpus","text":"Some special arguments need to be passed to Docker to enable it to use your GPU. With Docker version > 19.03 You can use docker run --gpus all to expose all GPUs to the container. See the NVIDIA Docker documentation for more details. If using CUDA, GaNDLF also expects the environment variable CUDA_VISIBLE_DEVICES to be set. To use the same settings as your host machine, simply add -e CUDA_VISIBLE_DEVICES to your docker run command. For example: For example: ( main ) $> docker run --gpus all -e CUDA_VISIBLE_DEVICES -it --rm --name gandlf cbica/gandlf:latest-cuda113 gandlf run --device cuda [ ... ] This can be replicated for ROCm for AMD , by following the instructions to set up the ROCm Container Toolkit .","title":"Enabling GPUs"},{"location":"usage/#mlcubes","text":"GaNDLF, and GaNDLF-created models, may be distributed as an MLCube . This involves distributing an mlcube.yaml file. That file can be specified when using the MLCube runners . The runner will perform many aspects of configuring your container for you. Currently, only the mlcube_docker runner is supported. See the MLCube documentation for more details.","title":"MLCubes"},{"location":"usage/#huggingface-cli","text":"This tool allows you to interact with the Hugging Face Hub directly from a terminal. For example, you can create a repository, upload and download files, etc.","title":"HuggingFace CLI"},{"location":"usage/#download-an-entire-repository","text":"GaNDLF's Hugging Face CLI allows you to download repositories through the command line. This can be done by just specifying the repo id: ( main ) $> gandlf hf --download --repo-id HuggingFaceH4/zephyr-7b-beta Apart from the Repo Id you can also provide other arguments.","title":"Download an entire repository"},{"location":"usage/#revision","text":"To download from a specific revision (commit hash, branch name or tag), use the --revision option: ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1","title":"Revision"},{"location":"usage/#specify-a-token","text":"To access private or gated repositories, you must use a token. You can do this using the --token option: ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1 --token hf_****","title":"Specify a token"},{"location":"usage/#specify-cache-directory","text":"If not using --local-dir, all files will be downloaded by default to the cache directory defined by the HF_HOME environment variable. You can specify a custom cache using --cache-dir: ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1 --token hf_**** --cache-dir ./path/to/cache","title":"Specify cache directory"},{"location":"usage/#download-to-a-local-folder","text":"The recommended (and default) way to download files from the Hub is to use the cache-system. However, in some cases you want to download files and move them to a specific folder. This is useful to get a workflow closer to what git commands offer. You can do that using the --local-dir option. A ./huggingface/ folder is created at the root of your local directory containing metadata about the downloaded files. This prevents re-downloading files if they\u2019re already up-to-date. If the metadata has changed, then the new file version is downloaded. This makes the local-dir optimized for pulling only the latest changes. ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1 --token hf_**** --cache-dir ./path/to/cache --local-dir ./path/to/dir","title":"Download to a local folder"},{"location":"usage/#force-download","text":"To specify if the files should be downloaded even if it already exists in the local cache. ( main ) $> gandlf hf --download --repo-id distilbert-base-uncased revision --revision v1.1 --token hf_**** --cache-dir ./path/to/cache --local-dir ./path/to/dir --force-download","title":"Force Download"},{"location":"usage/#upload-an-entire-folder","text":"Use the gandlf hf --upload upload command to upload files to the Hub directly. ( main ) $> gandlf hf --upload --repo-id Wauplin/my-cool-model --folder-path ./model --token hf_****","title":"Upload an entire folder"},{"location":"usage/#upload-to-a-specific-path-in-repo","text":"Relative path of the directory in the repo. Will default to the root folder of the repository. ( main ) $> gandlf hf --upload --repo-id Wauplin/my-cool-model --folder-path ./model/data --path-in-repo ./data --token hf_****","title":"Upload to a Specific Path in Repo"},{"location":"usage/#upload-multiple-files","text":"To upload multiple files from a folder at once without uploading the entire folder, use the --allow-patterns and --ignore-patterns patterns. It can also be combined with the --delete-patterns option to delete files on the repo while uploading new ones. In the example below, we sync the local Space by deleting remote files and uploading all files except the ones in /logs: ( main ) $> gandlf hf Wauplin/space-example --repo-type = space --exclude = \"/logs/*\" --delete = \"*\" --commit-message = \"Sync local Space with Hub\"","title":"Upload multiple files"},{"location":"usage/#specify-a-token_1","text":"To upload files, you must use a token. By default, the token saved locally will be used. If you want to authenticate explicitly, use the --token option: ( main ) $>gandlf hf --upload Wauplin/my-cool-model --folder-path ./model --token = hf_****","title":"Specify a token"},{"location":"usage/#specify-a-commit-message","text":"Use the --commit-message and --commit-description to set a custom message and description for your commit instead of the default one ( main ) $>gandlf hf --upload Wauplin/my-cool-model --folder-path ./model --token = hf_**** --commit-message \"Epoch 34/50\" --commit-description = \"Val accuracy: 68%. Check tensorboard for more details.\"","title":"Specify a commit message"},{"location":"usage/#upload-to-a-dataset-or-space","text":"To upload to a dataset or a Space, use the --repo-type option: ( main ) $>gandlf hf --upload Wauplin/my-cool-model --folder-path ./model --token = hf_**** --repo-type dataset","title":"Upload to a dataset or Space"},{"location":"usage/#huggingface-template-for-upload","text":"","title":"Huggingface Template For Upload"},{"location":"usage/#design-and-modify-template","text":"To design the huggingface template use the hugging_face.md file change the mandatory field [REQUIRED_FOR_GANDLF] to it's respective name don't leave it blank other wise it may through error, other field can be modeified by the user as per his convenience # Here the required field change from [REQUIRED_FOR_GANDLF] to [GANDLF] **Developed by:** {{ developers | default ( \"[GANDLF]\" , true )}}","title":"Design and Modify Template"},{"location":"usage/#mentioned-the-huggingface-template","text":"To mentioned the Huggingface Template , use the --hf-template option: ( main ) $>gandlf hf --upload Wauplin/my-cool-model --folder-path ./model --token = hf_**** --repo-type dataset --hf-template /hugging_face.md","title":"Mentioned The Huggingface Template"}]}
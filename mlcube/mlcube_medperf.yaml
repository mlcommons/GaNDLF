## This YAML file contains MLCube configuration.
## The gandlf_deploy tool looks for this file to generate your embedded-model MLCube image.
## If you are a model author, this file (or the derivative generated by gandlf_deploy)
## can be distributed along with your model container to enable use as an MLCube.
## See the MLCube specifications (ex: https://mlcommons.github.io/mlcube/runners/) for additional options.


# Metadata. If you are a model author, change this to reflect your model and organization.
name: MLCommons GaNDLF Medperf MLCube
description: MLCommons GaNDLF MLCube, containing functionality for model training, inference and data construction. Adapted for Medperf use.
authors: 
 - {name: "MLCommons Medical Working Group", email: "medical@mlcommons.org", org: "MLCommons" }

## Specifies hardware and software requirements.
platform:
  # Change this to some value >0 to use GPU(s) when running
  accelerator_count: 0
  # Some other sample platform requirement parameters
  #  accelerator_maker: NVIDIA
  #  accelerator_model: A100-80GB
  #  host_memory_gb: 40
  #  need_internet_access: True
  #  host_disk_space_gb: 100

docker:
  # The image tag that will be built/pulled/used. Change to suit your organization/model:version.
  image: mlcommons/gandlf:0.0.1
  
  ## Generally, these build options will only be needed by GaNDLF maintainers.
  # Docker build context relative to $MLCUBE_ROOT. (gandlf_deploy can handle this automatically.)
  build_context: "../"
  # Docker file name within docker build context. Any "Dockerfile-*" in the GaNDLF source repo is valid.
  build_file: "Dockerfile-CUDA11.3"
  
  # These settings should usually be set by global MLCube configuration, generally not per-deployment.
  # However, some sane defaults (for Docker >19.03) are here:
  #env_args:
  #  CUDA_VISIBLE_DEVICES: "${oc.env:CUDA_VISIBLE_DEVICES}"
  #gpu_args: --gpus all
  

singularity:
  # Image name. Change to suit your organization/model/version
  image: mlcommons-gandlf-0.0.1.simg

## Everything below this point affects how the GaNDLF container is invoked.
## If you are a model author, it is strongly recommended that you do not edit these.
## Please request any new features for deployed containers from the GaNDLF maintainers:
## https://github.com/mlcommons/GaNDLF/issues/new?template=---feature-request.md

tasks:
  train:
  # Trains a new model, creating a model directory, or resumes training on an existing model.
    entrypoint: "python3.8 gandlf_run --train True --device cpu"
    parameters:
      inputs: {
        # Path to a data csv such as that constructed by the "construct_csv" task.
        data_path: {type: "directory", default: "data/"},
        # Path to a GaNDLF config file. See samples for more examples.
        parameters_file: {type: "file", default: "config.yml"},
      }
      outputs: {
        # Path to a model directory. Not used if deploying an embedded model.
        output_path: {type: "directory", default: "model/"},
      }
      
      
  infer:
  # Runs inference on some existing model given new data
    entrypoint: "python3.8 gandlf_run --train False --device cpu"
    parameters:
      inputs: {
        # Path to a data csv such as that constructed by the "construct_csv" task.
        data_path: {type: "directory", default: "data/"},
        # Path to a GaNDLF config file. See samples for more examples.
        #parameters_file: {type: "file", default: "config.yml"},
        # Path to a model directory. Not used if deploying an embedded model.
        modeldir: {type: "directory", default: "additional_files/model/"},
        #device: {type: "str", default: "cpu"},
        #parameters_file: {type: file, default: parameters.yaml}
      }
      outputs: {output_path: {type: "directory", default: "inference_results"}}

# There are two construct_csv tasks below to make it more convenient for users to specify inference and training datasets.

  construct_csv:
  # Constructs a data csv from a data directory that can be passed to future steps, to prevent issues with path translation between host and container.
    entrypoint: "python3.8 gandlf_constructCSV --relativizePaths True"
    parameters:
      inputs: {
        # Do NOT change the position of the inputDir parameter! It is relevant due to MLCube mounting rules.
        # Path to a directory containing input data. Each subject should be a subdirectory, with consistent filenaming conventions.
        inputDir: {type: "directory", default: "data/"},
        # Path to a file containing identifying strings for each channel (and label, if performing segmentation).
        channelsID: {type: "file", default: "channelIDs.yml"},
      }
      outputs: {
        outputFile: {type: "file", default: "data.csv"}
        }
    
  recover_config:
  # Extracts the config file from the embedded model (if any) in the MLCube.
    entrypoint: "python3.8 gandlf_recoverConfig --mlcube internal"
    parameters: 
      outputs: {
        outputFile: {type: "file", default: "recovered_config.yml"},
      }